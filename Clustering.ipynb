{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import Request, urlopen\n",
    "\n",
    "from bs4 import BeautifulSoup  #parse html page\n",
    "def getcorpus(url,links):\n",
    "    request = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    response = urlopen(request)\n",
    "    soup = BeautifulSoup(response, \"lxml\")\n",
    "    for a in soup.findAll('a'): #find links\n",
    "        try:\n",
    "            url = a['href']\n",
    "            title = a['title']\n",
    "            if title == \"Older Posts\":\n",
    "                print(title, url)\n",
    "                links.append(url)\n",
    "                getcorpus(url, links)\n",
    "        except:\n",
    "            title=\"\"\n",
    "    return title\n",
    "blog = \"https://blog.openai.com/\"\n",
    "links = []\n",
    "getcorpus(blog, links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://blog.openai.com/block-sparse-gpu-kernels/',\n",
       " 'https://blog.openai.comhttps://twitter.com/openai',\n",
       " 'https://blog.openai.comhttps://www.facebook.com/openai.research',\n",
       " 'https://blog.openai.com/block-sparse-gpu-kernels/',\n",
       " 'https://blog.openai.com/learning-a-hierarchy/',\n",
       " 'https://blog.openai.com/generalizing-from-simulation/',\n",
       " 'https://blog.openai.com/meta-learning-for-wrestling/',\n",
       " 'https://blog.openai.com/competitive-self-play/',\n",
       " 'https://blog.openai.com/nonlinear-computation-in-linear-networks/',\n",
       " 'https://blog.openai.com/learning-to-model-other-minds/',\n",
       " 'https://blog.openai.com/baselines-acktr-a2c/',\n",
       " 'https://blog.openai.com/more-on-dota-2/',\n",
       " 'https://blog.openai.com/dota-2/',\n",
       " 'https://blog.openai.com/gathering_human_feedback/',\n",
       " 'https://blog.openai.com/better-exploration-with-parameter-noise/',\n",
       " 'https://blog.openai.com/openai-baselines-ppo/',\n",
       " 'https://blog.openai.com/subscribe',\n",
       " 'https://blog.openai.comhttps://twitter.com/openai',\n",
       " 'https://blog.openai.comhttps://www.facebook.com/openai.research',\n",
       " 'https://blog.openai.com/robust-adversarial-inputs/',\n",
       " 'https://blog.openai.com/faster-robot-simulation-in-python/',\n",
       " 'https://blog.openai.com/deep-reinforcement-learning-from-human-preferences/',\n",
       " 'https://blog.openai.com/learning-to-cooperate-compete-and-communicate/',\n",
       " 'https://blog.openai.com/openai-baselines-dqn/',\n",
       " 'https://blog.openai.com/robots-that-learn/',\n",
       " 'https://blog.openai.com/roboschool/',\n",
       " 'https://blog.openai.com/unsupervised-sentiment-neuron/',\n",
       " 'https://blog.openai.com/spam-detection-in-the-physical-world/',\n",
       " 'https://blog.openai.com/evolution-strategies/',\n",
       " 'https://blog.openai.com/distill/',\n",
       " 'https://blog.openai.com/learning-to-communicate/',\n",
       " 'https://blog.openai.com/adversarial-example-research/',\n",
       " 'https://blog.openai.com/team-update-january/',\n",
       " 'https://blog.openai.com/faulty-reward-functions/',\n",
       " 'https://blog.openai.com/universe/',\n",
       " 'https://blog.openai.com/openai-and-microsoft/',\n",
       " 'https://blog.openai.com/report-from-the-self-organizing-conference/',\n",
       " 'https://blog.openai.com/infrastructure-for-deep-learning/',\n",
       " 'https://blog.openai.com/machine-learning-unconference/',\n",
       " 'https://blog.openai.com/team-update-august/',\n",
       " 'https://blog.openai.com/special-projects/',\n",
       " 'https://blog.openai.com/concrete-ai-safety-problems/',\n",
       " 'https://blog.openai.com/openai-technical-goals/',\n",
       " 'https://blog.openai.com/generative-models/',\n",
       " 'https://blog.openai.com/team-update/',\n",
       " 'https://blog.openai.com/openai-gym-beta/',\n",
       " 'https://blog.openai.com/welcome-pieter-and-shivon/',\n",
       " 'https://blog.openai.com/team-plus-plus/',\n",
       " 'https://blog.openai.com/introducing-openai/',\n",
       " 'https://blog.openai.comhttps://twitter.com/openai',\n",
       " 'https://blog.openai.comhttps://www.facebook.com/openai.research']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = []\n",
    "def getcorpus(url):\n",
    "    request = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    response = urlopen(request)\n",
    "    links = []\n",
    "    soup = BeautifulSoup(response, \"lxml\")\n",
    "    for a in soup.findAll('a'): #find links\n",
    "        url = a['href']\n",
    "        if \"openai.com\" not in url:\n",
    "            links.append(\"https://blog.openai.com\"+ url)\n",
    "\n",
    "    return links\n",
    "\n",
    "def getText(url):\n",
    "    request = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    response = urlopen(request)\n",
    "    soup = BeautifulSoup(response, \"lxml\")\n",
    "    mydivs = soup.findAll(\"div\", {\"class\":\"MainContent\"})\n",
    "    posts = []\n",
    "    for div in mydivs:\n",
    "        posts+=map(lambda p:p.text.encode('ascii', errors='replace').decode('ascii').replace(\"?\",\" \"), div.findAll(\"p\"))\n",
    "    return (''.join(posts)) #return article as a string\n",
    "blog = \"https://blog.openai.com/\"\n",
    "links = []\n",
    "links = getcorpus(blog)\n",
    "links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://blog.openai.comhttps://twitter.com/openai\n",
      "https://blog.openai.comhttps://www.facebook.com/openai.research\n",
      "https://blog.openai.comhttps://twitter.com/openai\n",
      "https://blog.openai.comhttps://www.facebook.com/openai.research\n",
      "https://blog.openai.comhttps://twitter.com/openai\n",
      "https://blog.openai.comhttps://www.facebook.com/openai.research\n"
     ]
    }
   ],
   "source": [
    "text = []\n",
    "for link in links:\n",
    "    try:\n",
    "        text.append(getText(link))\n",
    "    except:\n",
    "        print(link)\n",
    "        \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"We re releasing highly-optimized GPU kernels for an underexplored class of neural network architectures: networks with block-sparse weights. Depending on the chosen sparsity, these kernels can run orders of magnitude faster than cuBLAS or cuSPARSE. We've used them to attain state-of-the-art results in text sentiment analysis and generative modeling of text and images.The development of model architectures and algorithms in the field of deep learning is largely constrained by the availability of efficient GPU implementations of elementary operations. One issue has been the lack of an efficient GPU implementation for sparse linear operations, which we're now releasing, together with initial results using them to implement a number of sparsity patterns. These initial results are promising but not definitive, and we invite the community to join us in pushing the limits of the architectures these kernels unlock.\\nDense layers (left) can be replaced with layers that are sparse and wide (center) or sparse and deep (right) while approximately retaining computation time.Sparse weight matrices, as opposed to dense weight matrices, have a large number of entries with a value of exactly zero. Sparse weight matrices are attractive as building blocks of models, since the computational cost of matrix multiplication and convolution with sparse blocks is only proportional to the number of non-zero blocks. Sparsity enables, for example, training of neural networks that are much wider and deeper than otherwise possible with a given parameter budget and computational budget, such as LSTMs with tens of thousands of hidden units. (The largest LSTMs trained today are only thousands of hidden units.)\\nVisualization of dense (left) and block-sparse (center) weight matrices, where white indicates a weight of zero.The kernels allow efficient usage of block-sparse weights in fully connected and convolutional layers (shown above). For convolutional layers, the kernels allow for sparsity in input and output feature dimensions; the connectivity is unaffected in the spatial dimensions. The sparsity is defined at the level of blocks (right figure above), and have been optimized for block sizes of 8x8 (such as in this example), 16x16 or 32x32. At the block level, the sparsity pattern is completely configurable. Since the kernels skip computations of blocks that are zero, the computational cost is only proportional to the number of non-zero weights, not the number of input/output features. The cost for storing the parameters is also only proportional to the number of non-zero weights.\\nSpeed-up factor for various levels of sparsity, compared to cuBLAS, when used with a wide state (12288 hidden units), block size of 32x32 and minibatch size of 32. Comparison was done on a NVIDIA Titan X Pascal GPU with CUDA 8. Speed-ups compared to cuSPARSE were even larger for the tested levels of sparsity.Below we show some example code for performing sparse matrix multiplication in Tensorflow.One particularly interesting use of block-sparse kernels is to use them to create small-world neural networks. Small-world graphs are connected in such a way that any two nodes in the graph are connected via a small number of steps, even if the graph has billions of nodes. Our motivation for implementing small world connectivity, is despite having a high degree of sparsity, we still want information to propagate quickly through the network. Brains display small-world connectivity patterns, which prompts the question whether the same property can improve the performance of LSTMs. Using small-world sparse connectivity, we efficiently trained LSTMs with almost twenty thousands hidden units, 5 times wider than a dense network with similar parameter counts, improving results on generative modeling of text, and semi-supervised sentiment classification; see our paper for more details.\\nIn small-world graphs, even with high degrees of sparsity, nodes are connected in in small number of steps. The animation above shows spreading activation from the center node (pixel) in a two-dimensional Watts-Strogatz small-world graph, stochastically smoothed for aesthetic purposes. In this graph, the average path length between nodes is less than 5, similar to the Barabasi-Albert graphs used in our LSTM experiments.Following the setup we used in our sentiment neuron experiment, we trained LSTMs with approximately equivalent parameter counts and compared models with dense weight matrices against a block-sparse variant. The sparse model outperforms the dense model on all sentiment datasets. Our sparse model improves the state of the art on the document level IMDB dataset from 5.91% error (Miyato et al, 2016) to 5.01%. This is a promising improvement over our previous results which performed best only on shorter sentence level datasets.Sentiment classification error (%) of linear models trained on the features of dense and sparse generative models with approximately equivalent total parameter counts.By using sparse and wide LSTMs, the bits-per-character results in our experiments dropped from 1.059 to 1.048, for equal parameter counts (~ 100 million). Architectures with block-sparse linear layers can also improve upon results obtained with densely connected linear layers. We performed a simple modification of the PixelCNN++ model of CIFAR-10 natural images. A replacement of regular 2D convolutional kernels with sparse kernels, while deepening the network but keeping the rest of the hyper-parameters fixed, lead to a drop in the bits-per-dimension from 2.92 to 2.90, now state of the art on this dataset.Here we list some suggestions for future research.\",\n",
       " \"We re releasing highly-optimized GPU kernels for an underexplored class of neural network architectures: networks with block-sparse weights. Depending on the chosen sparsity, these kernels can run orders of magnitude faster than cuBLAS or cuSPARSE. We've used them to attain state-of-the-art results in text sentiment analysis and generative modeling of text and images.The development of model architectures and algorithms in the field of deep learning is largely constrained by the availability of efficient GPU implementations of elementary operations. One issue has been the lack of an efficient GPU implementation for sparse linear operations, which we're now releasing, together with initial results using them to implement a number of sparsity patterns. These initial results are promising but not definitive, and we invite the community to join us in pushing the limits of the architectures these kernels unlock.\\nDense layers (left) can be replaced with layers that are sparse and wide (center) or sparse and deep (right) while approximately retaining computation time.Sparse weight matrices, as opposed to dense weight matrices, have a large number of entries with a value of exactly zero. Sparse weight matrices are attractive as building blocks of models, since the computational cost of matrix multiplication and convolution with sparse blocks is only proportional to the number of non-zero blocks. Sparsity enables, for example, training of neural networks that are much wider and deeper than otherwise possible with a given parameter budget and computational budget, such as LSTMs with tens of thousands of hidden units. (The largest LSTMs trained today are only thousands of hidden units.)\\nVisualization of dense (left) and block-sparse (center) weight matrices, where white indicates a weight of zero.The kernels allow efficient usage of block-sparse weights in fully connected and convolutional layers (shown above). For convolutional layers, the kernels allow for sparsity in input and output feature dimensions; the connectivity is unaffected in the spatial dimensions. The sparsity is defined at the level of blocks (right figure above), and have been optimized for block sizes of 8x8 (such as in this example), 16x16 or 32x32. At the block level, the sparsity pattern is completely configurable. Since the kernels skip computations of blocks that are zero, the computational cost is only proportional to the number of non-zero weights, not the number of input/output features. The cost for storing the parameters is also only proportional to the number of non-zero weights.\\nSpeed-up factor for various levels of sparsity, compared to cuBLAS, when used with a wide state (12288 hidden units), block size of 32x32 and minibatch size of 32. Comparison was done on a NVIDIA Titan X Pascal GPU with CUDA 8. Speed-ups compared to cuSPARSE were even larger for the tested levels of sparsity.Below we show some example code for performing sparse matrix multiplication in Tensorflow.One particularly interesting use of block-sparse kernels is to use them to create small-world neural networks. Small-world graphs are connected in such a way that any two nodes in the graph are connected via a small number of steps, even if the graph has billions of nodes. Our motivation for implementing small world connectivity, is despite having a high degree of sparsity, we still want information to propagate quickly through the network. Brains display small-world connectivity patterns, which prompts the question whether the same property can improve the performance of LSTMs. Using small-world sparse connectivity, we efficiently trained LSTMs with almost twenty thousands hidden units, 5 times wider than a dense network with similar parameter counts, improving results on generative modeling of text, and semi-supervised sentiment classification; see our paper for more details.\\nIn small-world graphs, even with high degrees of sparsity, nodes are connected in in small number of steps. The animation above shows spreading activation from the center node (pixel) in a two-dimensional Watts-Strogatz small-world graph, stochastically smoothed for aesthetic purposes. In this graph, the average path length between nodes is less than 5, similar to the Barabasi-Albert graphs used in our LSTM experiments.Following the setup we used in our sentiment neuron experiment, we trained LSTMs with approximately equivalent parameter counts and compared models with dense weight matrices against a block-sparse variant. The sparse model outperforms the dense model on all sentiment datasets. Our sparse model improves the state of the art on the document level IMDB dataset from 5.91% error (Miyato et al, 2016) to 5.01%. This is a promising improvement over our previous results which performed best only on shorter sentence level datasets.Sentiment classification error (%) of linear models trained on the features of dense and sparse generative models with approximately equivalent total parameter counts.By using sparse and wide LSTMs, the bits-per-character results in our experiments dropped from 1.059 to 1.048, for equal parameter counts (~ 100 million). Architectures with block-sparse linear layers can also improve upon results obtained with densely connected linear layers. We performed a simple modification of the PixelCNN++ model of CIFAR-10 natural images. A replacement of regular 2D convolutional kernels with sparse kernels, while deepening the network but keeping the rest of the hyper-parameters fixed, lead to a drop in the bits-per-dimension from 2.92 to 2.90, now state of the art on this dataset.Here we list some suggestions for future research.\",\n",
       " \"We've developed a hierarchical reinforcement learning algorithm that learns high-level actions useful for solving a range of tasks, allowing fast solving of tasks requiring thousands of timesteps. Our algorithm, when applied to a set of navigation problems, discovers a set of high-level actions for walking and crawling in different directions, which enables the agent to master new navigation tasks quickly.Humans solve complicated challenges by breaking them up into small, manageable components. Grilling pancakes consists of a series of high-level actions, such as measuring flour, whisking eggs, transferring the mixture to the pan, turning the stove on, and so on. Humans are able to learn new tasks rapidly by sequencing together these learned components, even though the task might take millions of low-level actions, i.e., individual muscle contractions.On the other hand, today s reinforcement learning methods operate through brute force search over low-level actions, requiring an enormous number of attempts to solve a new task. These methods become very inefficient at solving tasks that take a large number of timesteps.Our solution is based on the idea of hierarchical reinforcement learning, where agents represent complicated behaviors as a short sequence of high-level actions. This lets our agents solve much harder tasks: while the solution might require 2000 low-level actions, the hierarchical policy turns this into a sequence of 10 high-level actions, and it's much more efficient to search over the 10-step sequence than the 2000-step sequence.Our algorithm, meta-learning shared hierarchies (MLSH), learns a hierarchical policy where a master policy switches between a set of sub-policies. The master selects an action every every N timesteps, where we might take N=200. A sub-policy executed for N timesteps constitutes a high-level action, and for our navigation tasks, sub-policies correspond to walking or crawling in different directions.In most prior work, hierarchical policies have been explicitly hand-engineered. Instead, we aim to discover this hierarchical structure automatically through interaction with the environment. Taking a meta-learning perspective, we define a good hierarchy as one that quickly reaches high reward quickly when training on unseen tasks. Hence, the MLSH algorithm aims to learn sub-policies that enable fast learning on previously unseen tasks.We train on a distribution over tasks, sharing the sub-policies while learning a new master policy on each sampled task. By repeatedly training new master policies, this process automatically finds sub-policies that accommodate  the master policy's learning dynamics.\\nAfter running overnight, an MLSH agent trained to solve nine separate mazes discovered sub-policies corresponding to upwards, rightwards, and downwards movement, which it then used to navigate its way through the mazes.In our AntMaze environment, a Mujoco Ant robot is placed into a distribution of 9 different mazes and must navigate from the starting position to the goal. Our algorithm is successfully able to find a diverse set of sub-policies that can be sequenced together to solve the maze tasks, solely through interaction with the environment. This set of sub-policies can then be used to master a larger task than the ones they were trained on (see video at beginning at post).\\nTraining on separate maze environments leads to the automatic learning of sub-policies to solve the task.We re releasing the code for training the MLSH agents, as well as the MuJoCo environments we built to evaluate these algorithms.\",\n",
       " \"Our latest robotics techniques allow robot controllers, trained entirely in simulation and deployed on physical robots, to react to unplanned changes in the environment as they solve simple tasks. That is, we've used these techniques to build closed-loop systems rather than open-loop ones as before. The simulator need not match the real-world in appearance or dynamics; instead, we randomize relevant aspects of the environment, from friction to action delays to sensor noise. Our new results provide more evidence that general-purpose robots can be built by training entirely in simulation, followed by a small amount of self-calibration in the real world.\\nThis robot was trained in simulation with dynamics randomization to push a puck to a goal. In the real world, we put the puck on a bag of chips, and the robot still achieves the goal despite the bag changing the sliding properties in an unfamiliar way.We developed dynamics randomization to train a robot to adapt to unknown real-world dynamics. During training, we randomize a large set of ninety-five properties that determine the dynamics of the environment, such as altering the mass of each link in the robot s body; the friction and damping of the object it is being trained on; the height of the table the object is on; the latency between actions; the noise in its observations; and so on.We used this approach to train an LSTM-based policy to push a hockey puck around a table. Our feed-forward networks fail at this task, whereas LSTMs can use their past observations to analyze the dynamics of the world and adjust their behavior accordingly.We also trained a robot end-to-end in simulation using reinforcement learning (RL), and deployed the resulting policy on a physical robot. The resulting system maps vision directly to action without special sensors, and can adapt to visual feedback.\\nView from the robot's camera. The policy for picking up blocks is trained end-to-end from vision to action, on randomized visuals. Note that in simulation, the gripper moves up slightly as the block slides down in order to keep the block in the desired position. The gripper doesn't do this in the physical world as the block does not slide.The abundance of RL results with simulated robots can make it seem like RL easily solves most robotics tasks. But common RL algorithms work well only on tasks where small perturbations to your action can provide an incremental change to the reward. Some robotics tasks have simple rewards, like walking, where you can be scored on distance traveled. But most tasks do not   to define a dense reward for block stacking, you'd need to encode that the arm is close to the block, that the arm approaches the block in the correct orientation, that the block is lifted off the ground, the distance of block to the desired position, etc.We spent a number of months unsuccessfully trying to get conventional RL algorithms working on pick-and-place tasks before ultimately developing a new reinforcement learning algorithm, Hindsight Experience Replay (HER), which allows agents to learn from a binary reward by pretending that a failure was what they wanted to do all along and learning from it accordingly. (By analogy, imagine looking for a gas station but ending up at a pizza shop. You still don't know where to get gas, but you've now learned where to get pizza.) We also used domain randomization on the visual shapes to learn a vision system robust enough for the physical world.Our HER implementation uses the actor-critic technique with asymmetric information. (The actor is the policy, and the critic is a network which receives action/state pairs and estimates their Q-value, or sum of future rewards, providing training signal to the actor.) While the critic has access to the full state of the simulator, the actor only has access to RGB and depth data. Thus the critic can provide fully accurate feedback, while the actor uses only data present in the real world.Both techniques increase the computational requirements: dynamics randomization slows training down by a factor of 3x, while learning from images rather than states is about 5-10x slower.We see three approaches to building general-purpose robots: training on huge fleets of physical robots, making simulators increasingly match the real world, and randomizing the simulator to allow the model to generalize to the real-world. We increasingly believe that the third will be the most important part of the solution.If you re interested in helping us push towards general-purpose robots, consider joining our team at OpenAI.\\n\\n\\nA naive feed-forward policy is unable to adapt to the real-world puck environment despite solving the task in simulation.\",\n",
       " \"We show that for the task of simulated robot wrestling, a meta-learning agent can learn to quickly defeat a stronger non-meta-learning agent, and also show that the meta-learning agent can adapt to physical malfunction.\\nA simulated robot ant (red) uses its meta-learning policy to learn to beat a stronger creature with more legs (green).We've extended the Model-Agnostic Meta-Learning (MAML) algorithm by basing its objective function on optimizing against pairs of environments, rather than single ones as in stock MAML. MAML initializes the policies of our agents so that after only a small number of parameter updates during execution on a new environment (or task) the agents learn to do better in that environment. The policy parameter updates at execution are done via gradient ascent steps on the reward collected during the few episodes of initial interaction with a new environment. By training on pairs we re able to create policies that quickly adapt to previously unseen environments, as long as the environment doesn t diverge too wildly from previous ones.To test our continuous adaptation approach we designed 3 types of agents   Ant (4-leg), Bug (6-leg), and Spider (8-leg)   and set up a multi-round game where each agent played several matches against the same opponent and adapted its policy parameters between the rounds to better counter the opponent's policy. In tests, we found that agents that could adapt their tactics are much better competitors than agents that have fixed policies. After training over a hundred agents, some of which learned fixed policies and others learned to adapt, we evaluated the fitness of each agent.Learning on the fly can also let agents deal with unusual changes in their own bodies as well, like adapting to some of their own limbs losing functionality over time. This suggests we can use techniques like this to develop agents that can handle both changes in their external environment and also changes in their own bodies or internal states.We're exploring meta-learning as part of our work on large-scale multi-agent research. Additionally, we're releasing the MuJoCo environments and trained policies used in this work so that others can experiment with these systems.\",\n",
       " 'We\\'ve found that self-play allows simulated AIs to discover physical skills like tackling, ducking, faking, kicking, catching, and diving for the ball, without explicitly designing an environment with these skills in mind. Self-play ensures that the environment is always the right difficulty for an AI to improve. Taken alongside our Dota 2 self-play results, we have increasing confidence that self-play will be a core part of powerful AI systems in the future.We set up competitions between multiple simulated 3D robots on a range of basic games, trained each agent with simple goals (push the opponent out of the sumo ring, reach the other side of the ring while preventing the other agent from doing the same, kick the ball into the net or prevent the other agent from doing so, and so on), then analyzed the different strategies that emerged.Agents initially receive dense rewards for behaviours that aid exploration like standing and moving forward, which are eventually annealed to zero in favor of being rewarded for just winning and losing. Despite the simple rewards, the agents learn subtle behaviors like tackling, ducking, faking, kicking and catching, and diving for the ball. Each agent s neural network policy is independently trained with Proximal Policy Optimization.To understand how complex behaviors can emerge through a combination of simple goals and competitive pressure, let\\'s analyze the sumo wrestling task. Here we took the dense reward defined in previous work for training a humanoid to walk, removed the term for velocity, added the negative L2 distance from the center of the ring and took this as a dense exploration reward for our sumo agents. The agents were allowed to use this reward for exploration in the ring initially, then we slowly annealed it to zero so the agents would learn to optimize for the competition reward   pushing the other player out of the ring   for the remaining training iterations.Though it is possible to design tasks and environments that require each of these skills, this requires effort and ingenuity on the part of human designers, and the agents\\' behaviors will be bounded in complexity by the problems that the human designer can pose for them. By developing agents through thousands of iterations of matches against successively better versions of themselves, we can create AI systems that successively bootstrap their own performance; we saw a similar phenomenon in our Dota 2 project, where self-play let us create an RL agent that could beat top human players at a solo version of the e-sport.Agent trained on sumo wrestling, and then subjected to a variable unknown \"wind\".These agents also exhibit transfer learning, applying skills learned in one setting to succeed in another never-before-seen one. In one case, we took the agent trained on the self-play sumo wrestling task and faced it with the task of standing while being perturbed by \"wind\" forces. The agent managed to stay upright despite never seeing the windy environment or observing wind forces, while agents trained to walk using classical reinforcement learning would fall over immediately.Agent trained to walk using classical reinforcement learning, and then subjected to a variable unknown \"wind\".Our agents were overfitting by co-learning policies that were precisely tailored to counter specific opponents, but would fail when facing new ones with different characteristics. We dealt with this by pitting each agent against several different opponents rather than just one. These possible opponents come from an ensemble of policies that were trained in parallel as well as policies from earlier in the training process. Given this diversity of opponents, agents needed to learn general strategies and not just ones targeted to a specific opponent.Additionally, we re releasing the MuJoCo environments and trained policies used in this work so that others can experiment with these systems. If you\\'d like to work on self-play systems, we\\'re hiring!',\n",
       " \"We've shown that deep linear networks   as implemented using floating-point arithmetic   are not actually linear and can perform nonlinear computation. We used evolution strategies to find parameters in linear networks that exploit this trait, letting us solve non-trivial problems.Neural networks consist of stacks of a linear layer followed by a nonlinearity like tanh or rectified linear unit. Without the nonlinearity, consecutive linear layers would be in theory mathematically equivalent to a single linear layer. So it's a surprise that floating point arithmetic is nonlinear enough to yield trainable deep networks.Numbers used by computers aren't perfect mathematical objects, but approximate representations using finite numbers of bits. Floating point numbers are commonly used by computers to represent mathematical objects. Each floating point number is represented by a combination of a fraction and an exponent. In the IEEE's float32 standard, 23 bits are used for the fraction and 8 for the exponent, and one for the sign.\\nImage Credit: WikipediaAs a consequence of these conventions and the binary format used, the smallest normal non-zero number (in binary) is 1.0..0 x 2^-126, which we refer to as min going forward. However, the next representable number is 1.0..01 x 2^-126, which we can write as min + 0.0..01 x 2^-126. It is evident that the gap between the 2nd number is by a factor of 2^20 smaller than gap between 0 and min. In float32, when numbers are smaller than the smallest representable number they get mapped to zero. Due to this 'underflow', around zero all computation involving floating point numbers becomes nonlinear.An exception to these restrictions is denormal numbers, which can be disabled on some computing hardware. While the GPU and cuBLAS have denormals enabled by default, TensorFlow builds all its primitives with denormals off (with the ftz=true flag set). This means that any non-matrix multiply operation written in TensorFlow has an implicit non-linearity following it (provided the scale of computation is near 1e-38).So, while in general the difference between any 'mathematical' number and their normal float representation is small, around zero there is a large gap and the approximation error can be very significant.This can lead to some odd effects where the familiar rules of mathematics stop applying. For instance, (a + b) x c becomes unequal to a x c + b x c.For example if you set a = 0.4 x min, b = 0.5 x min and c = 1 / min.Then: (a+b) x c = (0.4 x min + 0.5 x min) x 1 / min = (0 + 0) x 1 / min = 0.\\nHowever: (a x c) + (b x c) = 0.4 x min / min + 0.5 x min x 1 / min = 0.9.In another example, we can set a = 2.5 x min, b = -1.6 x min  and c = 1 x min.Then: (a+b) + c = (0) + 1 x min = min\\nHowever: (b+c) + a = (0 x min) + 2.5 x min = 2.5 x min.At this smallest scale the fundamental addition operation has become nonlinear!We wanted to know if this inherent nonlinearity could be exploited as a computational nonlinearity, as this would let deep linear networks perform nonlinear computations. The challenge is that modern differentiation libraries are blind to these nonlinearities at the smallest scale. As such, it would be difficult or impossible to train a neural network to exploit them via backpropagation.We can use evolution strategies (ES) to estimate gradients without having to rely on symbolic differentiation. Using ES we can indeed exploit the near-zero behavior of float32 as a computational nonlinearity. When trained on MNIST a deep linear network trained via backpropagation achieves a training accuracy of 94% and a testing accuracy of 92%. In contrast, the same linear network can achieve >99% training and 96.7% test accuracy when trained with ES and ensuring that the activations are sufficiently small to be in the nonlinear range of float32. This increase in training performance is due to ES exploiting the nonlinearities in the float32 representation. These powerful nonlinearities allow any layer to generate novel features which are nonlinear combinations of lower level features.  Here is the network structure:Beyond MNIST, we think other interesting experiments could be extending this work to recurrent neural networks, or to exploit nonlinear computation to improve complex machine learning tasks like language modeling and translation. We're excited to explore this capability with our fellow researchers.\",\n",
       " \"We're releasing an algorithm which accounts for the fact that other agents are learning too, and discovers self-interested yet collaborative strategies like tit-for-tat in the iterated prisoner's dilemma. This algorithm, Learning with Opponent-Learning Awareness (LOLA), is a small step towards agents that model other minds.LOLA, a collaboration by researchers at OpenAI and the University of Oxford, lets an RL agent take account of the learning of others when updating its own strategy. Each LOLA agent adjusts its policy in order to shape the learning of the other agents in a way that is advantageous. This is possible since the learning of the other agents depends on the rewards and observations occurring in the environment, which in turn can be influenced by the agent.This means that the LOLA agent,  Alice , models how the parameter updates of the other agent,  Bob , depend on its own policy and how Bob's parameter update impacts its own future expected reward. Alice then updates its own policy in order to make the learning step of the other agents, like Bob, more beneficial to its own goals.LOLA agents can  discover effective, reciprocative strategies, in games like the iterated prisoner's dilemma, or the coin game. In contrast, state-of-the-art deep reinforcement learning methods, like Independent PPO, fail to learn such strategies in these domains. These agents typically learn to take selfish actions that ignore the objectives of other agents. LOLA solves this by letting agents act out of a self-interest that incorporates the goals of others. It also works without requiring hand-crafted rules, or environments set up to encourage cooperation.The inspiration for LOLA comes from how people collaborate with one another: Humans are great at reasoning about how their actions can affect the future behavior of other humans, and frequently invent ways to collaborate with others that leads to a 'win win'. One of the reasons humans are good at collaborating with each other is that they have a sense of a  theory of mind  about other humans, letting them come up with strategies that lead to benefits for their collaborators. So far, this sort of  theory of mind  representation has been absent from deep multi-agent reinforcement learning. To a state of the art deep-RL agent there is no inherent difference between another learning agent and a part of the environment, say a tree.The key to LOLA S performance is the inclusion of term:Here the left hand side captures how Alice s return depends on the change in Bob s policy. The right hand side,  describes how Bob s learning step depends on Alice s policy. Multiplying those two components essentially measures how Alice can change Bob s learning step such that it leads to an increase in Alice's rewards.This means that when we train our agents they try to optimize their return after one anticipated learning step of the opponent. By differentiating through this anticipated learning step the agent can actively shape the parameter update of the opponent in a way that increases their own return.While the formula above assume access to the true gradient and hessian of the two value functions, we can also estimate all relevant terms using samples. In particular the second order term can be estimated by applying the policy gradient theorem, which makes LOLA suitable for any deep reinforcement learning setting.LOLA can handle this by including a step of opponent modeling where we fit a model of the opponent to the observed trajectories - predicting the parameters of other agents based on their actions. In the future we would like to extend this by also inferring architectures and rewards from the observed learning.\\nLOLA works in situations where we can access the policy of the other agents (LOLA), as well as ones where we can only estimate the state of other agents from traces (LOLA-OM). Both approaches pick up a larger number of coins (left) and score a far greater number of points (right) than other approaches.LOLA lets us train agents that succeed at the coin game, in which two agents, red and blue, compete with one another to pick up red and blue colored coins. Each agent gets a point for picking up any coin, but if they pick up a coin which isn't their color then the other agent will receive a -2 penalty. Thus, if both agents greedily pick up both coins, everyone gets zero points on average. LOLA agents learn to predominantly pick up coins of their own color, leading to high scores (shown above).LOLA works best when using large batch-sizes and full roll-outs for variance reduction. This means that the method is both memory and compute intensive. Furthermore, under opponent modeling LOLA can exhibit instability which we hope to address with future improvements.\",\n",
       " 'We\\'re releasing two new OpenAI Baselines implementations: ACKTR and A2C. A2C is a synchronous, deterministic variant of Asynchronous Advantage Actor Critic (A3C) which we\\'ve found gives equal performance. ACKTR is a more sample-efficient reinforcement learning algorithm than TRPO and A2C, and requires only slightly more computation than A2C per update.ACKTR can learn continuous control tasks, like moving a robotic arm to a target location, purely from low-resolution pixel inputs (left).ACKTR (pronounced \"actor\")   Actor Critic using Kronecker-factored Trust Region   was developed by researchers at the University of Toronto and New York University, and we at OpenAI have collaborated with them to release a Baselines implementation. The authors use ACKTR to learn control policies for simulated robots (with pixels as input, and continuous action spaces) and Atari agents (with pixels as input and discrete action spaces).ACKTR combines three distinct techniques: actor-critic methods, trust region optimization for more consistent improvement, and distributed Kronecker factorization to improve sample efficiency and scalability.For machine learning algorithms, two costs are important to consider: sample complexity and computational complexity. Sample complexity refers to the number of timesteps of interaction between the agent and its environment, and computational complexity refers to the amount of numerical operations that must be performed.ACKTR has better sample complexity than first-order methods such as A2C because it takes a step in the natural gradient direction, rather than the gradient direction (or a rescaled version as in ADAM). The natural gradient gives us the direction in parameter space that achieves the largest (instantaneous) improvement in the objective per unit of change in the output distribution of the network, as measured using the KL-divergence. By limiting the KL divergence, we ensure that the new policy does not behave radically differently than the old one, which could cause a collapse in performance.As for computational complexity, the KFAC update used by ACKTR is only 10-25% more expensive per update step than a standard gradient update. This contrasts with methods like TRPO (i.e, Hessian-free optimization), which requires a more expensive conjugate-gradient computation.In the following video you can see comparisons at different timesteps between agents trained with ACKTR to solve the game Q-Bert and those trained with A2C. ACKTR agents get higher scores than ones trained with A2C.This release includes an OpenAI baseline release of ACKTR, as well as a release of A2C.We\\'re also publishing benchmarks that evaluate ACKTR against A2C, PPO and ACER on a range of tasks. In the following plot we show performance of ACKTR on 49 Atari games compared to other algorithm: A2C, PPO, ACER. The hyperparameters of ACKTR were tuned by the author of ACKTR solely on one game, Breakout.\\nACKTR performance also scales well with batch size because it not only derives a gradient estimate from the information in each batch, but also uses the information to approximate the local curvature in the parameter space. This feature is particularly favorable for large scale distributed training, in which large batch sizes are used.\\nThe Asynchronous Advantage Actor Critic method (A3C) has been very influential since the paper was published. The algorithm combines a few key ideas:After reading the paper, AI researchers wondered whether the asynchrony led to improved performance (e.g. \"perhaps the added noise would provide some regularization or exploration \"), or if it was just an implementation detail that allowed for faster training with a CPU-based implementation.As an alternative to the asynchronous implementation, researchers found you can write a synchronous, deterministic implementation that waits for each actor to finish its segment of experience before performing an update, averaging over all of the actors. One advantage of this method is that it can more effectively use of GPUs, which perform best with large batch sizes. This algorithm is naturally called A2C, short for advantage actor critic. (This term has been used in several papers.)Our synchronous A2C implementation performs better than our asynchronous implementations   we have not seen any evidence that the noise introduced by asynchrony provides any performance benefit. This A2C implementation is more cost-effective than A3C when using single-GPU machines, and is faster than a CPU-only A3C implementation when using larger policies.We have included code in Baselines for training feedforward convnets and LSTMs on the Atari benchmark using A2C.',\n",
       " 'Our Dota 2 result shows that self-play can catapult the performance of machine learning systems from far below human level to superhuman, given sufficient compute. In the span of a month, our system went from barely matching a high-ranked player to beating the top pros and has continued to improve since then. Supervised deep learning systems can only be as good as their training datasets, but in self-play systems, the available data improves automatically as the agent gets better.TrueSkill rating (similar to the ELO rating in chess) of our best bot over time, computed by simulating games between the bots and observing the win ratios. Improvements came from every part of the system, from adding new features to algorithmic improvements to scaling things up. The graph is surprisingly linear, meaning the team improved the bot exponentially over time.The project\\'s timeline is the following. For some perspective, 15% of players are below 1.5k MMR; 58% of players are below 3k; 99.99% are below 7.5k.Bot playing versus SumaiL.The full game is 5v5, but 1v1 also appears in some tournaments. Our bot played under standard tournament rules   we did not add AI-specific simplifications to 1v1.The bot operated off the following interfaces:We whitelisted a few dozen item builds that bots could use, and picked one for evaluation. We also separately trained the initial creep block using traditional RL techniques, as it happens before the opponent appears.Our approach, combining small amounts of \"coaching\" with self-play, allowed us to massively improve our agent between the Monday and Thursday of The International. On Monday evening, Pajkatt won using an unusual item build (buying an early magic wand). We added this item build to the training whitelist.Around 1pm on Wednesday, we tested the latest bot. The bot would lose a bunch of health in the first wave. We thought perhaps we needed to roll back, but noticed further gameplay was amazing, and the first wave behavior was baiting the other bots to be aggressive towards it. Further self-play fixed the issue, as the bot learned to counter the baiting strategy. In the meanwhile, we stitched it together with Monday\\'s bot for the first wave only, and completed the process twenty minutes before Arteezy showed up at 4pm.After the Arteezy matches, we updated the creep block model, which increased TrueSkill by one point. Further training before Sumail\\'s match on Thursday increased TrueSkill by two points. Sumail pointed out that the bot had learned to cast razes out of the enemy\\'s vision. This was due to a mechanic we hadn\\'t known about: abilities cast outside of the enemy\\'s vision prevent the enemy from gaining a wand charge.Arteezy also played a match against our 7.5k semi-pro tester. Arteezy was winning the whole game, but our tester still managed to surprise him with a strategy he\\'d learned from the bot. Arteezy remarked afterwards that this was a strategy that Paparazi had used against him once and was not commonly practiced.Pajkatt beating Monday\\'s bot. Note he baits the bot into engaging, and uses regeneration (faerie fires and a magic wand) to heal up. The bot is generally very good at deciding who will win a fight, but it\\'s never played against someone with early wand before.Though Sumail called the bot \"unbeatable\", it can still be confused in situations very different from what it\\'s seen. We set up the bot at a LAN event at The International, where players played over 1,000 games to beat the bot by any means possible.The successful exploits fell into three archetypes:Fixing these issues for 1v1 would be similar to fixing the Pajkatt bug. But for 5v5, such issues aren\\'t exploits at all, and we\\'ll need a system which can handle totally weird and wacky situations it\\'s never seen.We re not ready to talk about agent internals   the team is focused on solving 5v5 first.The first step in the project was figuring out how to run Dota 2 in the cloud on a physical GPU. The game gave an obscure error message on GPU cloud instances. But when starting it on Greg\\'s personal GPU desktop (which is the desktop brought onstage during the show), we noticed that Dota booted when the monitor was plugged in, but gave the same error message when unplugged. So we configured our cloud GPU instances to pretend there was a physical monitor attached.Dota didn\\'t support custom dedicated servers at the time, meaning that running scalably and without a GPU was possible only with very slow software rendering. We then created a shim to stub out most OpenGL calls, except the ones needed to boot.At the same time, we wrote a scripted bot   we needed a baseline for comparison (especially because the builtin bots don\\'t work well on 1v1) and to understand all the semantics of the bot API. The scripted bot reaches 70 last hits in ten minutes on an empty lane, but still loses to reasonable humans. Our current best 1v1 bot reaches more like 97 (it destroys the tower before then, so we can only extrapolate), and the theoretical maximum is 101.Bot playing versus SirActionSlacks. The strategy of distracting the bot with a courier rush did not work.1v1 is complicated, but 5v5 is an ocean of complexity. We know we\\'ll need to further push the limits of AI in order to solve it.One well-established place to start is with behavioral cloning. Dota has about a million public matches a day. The replays for these matches are stored on Valve\\'s servers for two weeks. We\\'ve been downloading every expert-level replay since last November, and have amassed a dataset of 5.8M games (each game is about 45 minutes with 10 humans). We use OpenDota to discover these replays, and are donating $12k (10 years of their fundraising goal) to support the project.We have many more ideas, and are hiring engineers (must be intrigued by machine learning, but need not be an expert) and researchers to help us make this happen. We thank Microsoft Azure and Valve for their support in this endeavor.',\n",
       " \"We've created a bot which beats the world's top professionals at 1v1 matches of Dota 2 under standard tournament rules. The bot learned the game from scratch by self-play, and does not use imitation learning or tree search. This is a step towards building AI systems which accomplish well-defined goals in messy, complicated situations involving real humans.Today we played Dendi on mainstage at The International, winning a best-of-three match. Over the past week, our bot was undefeated against many top professionals including SumaiL (top 1v1 player in the world) and Arteezy (top overall player in the world).Dota 1v1 is a complex game with hidden information. Agents must learn to plan, attack, trick, and deceive their opponents. The correlation between player skill and actions-per-minute is not strong, and in fact, our AI's actions-per-minute are comparable to that of an average human player.Success in Dota requires players to develop intuitions about their opponents and plan accordingly. In the above video you can see that our bot has learned   entirely via self-play   to predict where other players will move, to improvise in response to unfamiliar situations, and how to influence the other player's allied units to help it succeed.The full game of Dota is played by two teams of five. Each player chooses from a hundred heroes and hundreds of items. Our next step is to create a team of Dota 2 bots which can compete and collaborate with the top human teams. If you d like to work on the next phase of the project, consider joining OpenAI.\",\n",
       " \"RL-Teacher is an open-source implementation of our interface to train AIs via occasional human feedback rather than hand-crafted reward functions. The underlying technique was developed as a step towards safe AI systems, but also applies to reinforcement learning problems with rewards that are hard to specify.This simulated robot is being trained to do ballet via a human giving feedback. It's not obvious how to specify a reward function to achieve the same behavior.The release contains three main components:The entire system consists of less than 1,000 lines of Python code (excluding the agents). After you've set up your web server you can launch an experiment by running:Humans can give feedback via a simple web interface (shown above), which can be run locally (not recommended) or on a separate machine. Full documentation is available on the project's GitHub repository. We're excited to see what AI researchers and engineers do with this technology   please get in touch with any experimental results!\",\n",
       " \"We've found that adding adaptive noise to the parameters of reinforcement learning algorithms frequently boosts performance. This exploration method is simple to implement and very rarely decreases performance, so it's worth trying on any problem.Parameter noise helps algorithms more efficiently explore the range of actions available to solve an environment. After 216 episodes of training DDPG without parameter noise will frequently develop inefficient running behaviors, whereas policies trained with parameter noise often develop a high-scoring gallop.Parameter noise lets us teach agents tasks much more rapidly than with other approaches. After learning for 20 episodes on the HalfCheetah Gym environment (shown above), the policy achieves a score of around 3,000, whereas a policy trained with traditional action noise only achieves around 1,500.Parameter noise adds adaptive noise to the parameters of the neural network policy, rather than to its action space. Traditional RL uses action space noise to change the likelihoods associated with each action the agent might take from one moment to the next. Parameter space noise injects randomness directly into the parameters of the agent, altering the types of decisions it makes such that they always fully depend on what the agent currently senses. The technique is a middle ground between evolution strategies (where you manipulate the parameters of your policy but don t influence the actions a policy takes as it explores the environment during each rollout) and deep reinforcement learning approaches like TRPO, DQN, and DDPG (where you don t touch the parameters, but add noise to the action space of the policy).\\nAction space noise (left), compared to parameter space noise (right)Parameter noise helps algorithms explore their environments more effectively, leading to higher scores and more elegant behaviors. We think this is because adding noise in a deliberate manner to the parameters of the policy makes an agent's exploration consistent across different timesteps, whereas adding noise to the action space leads to more unpredictable exploration which isn t correlated to anything unique to the agent's parameters.People have previously tried applying parameter noise to policy gradients. We've extended this by showing that the technique works on policies based on deep neural networks and that it can be applied to both on- and off-policy algorithms.When conducting this research we ran into three problems:We use layer normalization to deal with the first problem, which ensures that the output of a perturbed layer (which will be the input to the next one) is still within a similar distribution.We tackle the second and third problem by introducing an adaptive scheme to adjust the size of the parameter space perturbations. This adjustment works by measuring the effect of the perturbation on action space and whether the action space noise level is larger or smaller than a defined target. This trick allows us to push the problem of choosing noise scale into action space, which is more interpretable than parameter space.We're also releasing baseline code that incorporates this technique for DQN, Double DQN, Dueling DQN, Dueling Double DQN, and DDPG.We've included benchmarks of the performance of DDQN with and without parameter noise on a subset of the Atari games corpus, and of three variants of DDPG on a range of continuous control tasks within the Mujoco simulator.When we first conducted this research, we found that the perturbations we applied to the Q function of DQN could sometimes be so extreme it would lead to the algorithm repeatedly executing the same action. To deal with this, we added a separate head that explicitly represents the policy as in DDPG (in regular DQN, the policy is only represented implicitly by the Q function) to make the setup more similar to our other experiments. However, when preparing our code for this release we ran an experiment that used parameter space noise without the separate policy head. We found that this worked comparably to our version with the separate policy head while being much simpler to implement. Further experiments confirmed that the separate policy head was indeed unnecessary as the algorithm had likely improved since our early experiments due to us altering how we re-scaled the noise. This led to a simpler, easier to implement, and less costly to train algorithm while still achieving very similar results. It s important to remember that AI algorithms, especially in reinforcement learning, can fail silently and subtly, which can lead to people engineering solutions around missed bugs.\",\n",
       " \"We re releasing a new class of reinforcement learning algorithms, Proximal Policy Optimization (PPO), which perform comparably or better than state-of-the-art approaches while being much simpler to implement and tune. PPO has become the default reinforcement learning algorithm at OpenAI because of its ease of use and good performance.PPO lets us train AI policies in challenging environments, like the Roboschool one shown above where an agent tries to reach a target (the pink sphere), learning to walk, run, turn, use its momentum to recover from minor hits, and how to stand up from the ground when it is knocked over.Policy gradient methods are fundamental to recent breakthroughs in using deep neural networks for control, from video games, to 3D locomotion, to Go. But getting good results via policy gradient methods is challenging because they are sensitive to the choice of stepsize   too small, and progress is hopelessly slow; too large and the signal is overwhelmed by the noise, or one might see catastrophic drops in performance. They also often have very poor sample efficiency, taking millions (or billions) of timesteps to learn simple tasks.Researchers have sought to eliminate these flaws with approaches like TRPO and ACER, by constraining or otherwise optimizing the size of a policy update. These methods have their own trade-offs   ACER is far more complicated than PPO, requiring the addition of code for off-policy corrections and a replay buffer, while only doing marginally better than PPO on the Atari benchmark; TRPO   though useful for continuous control tasks   isn't easily compatible with algorithms that share parameters between a policy and value function or auxiliary losses, like those used to solve problems in Atari and other domains where the visual input is significant.With supervised learning, we can easily implement the cost function, run gradient descent on it, and be very confident that we'll get excellent results with relatively little hyperparameter tuning. The route to success in reinforcement learning isn't as obvious -  the algorithms have many moving parts that are hard to debug, and they require substantial effort in tuning in order to get good results. PPO strikes a balance between ease of implementation, sample complexity, and ease of tuning, trying to compute an update at each step that minimizes the cost function while ensuring the deviation from the previous policy is relatively small.We've previously detailed a variant of PPO that uses an adaptive KL penalty to control the change of the policy at each iteration. The new variant uses a novel objective function not typically found in other algorithms:This objective implements a way to do a Trust Region update which is compatible with Stochastic Gradient Descent, and simplifies the algorithm by removing the KL penalty and need to make adaptive updates. In tests, this algorithm has displayed the best performance on continuous control tasks and almost matches ACER's performance on Atari, despite being far simpler to implement.Agents trained with PPO develop flexible movement policies that let them improvise turns and tilts as they head towards a target location.We've created interactive agents based on policies trained by PPO   we can use the keyboard to set new target positions for a robot in an environment within Roboschool; though the input sequences are different from what the agent was trained on, it manages to generalize.We've also used PPO to teach complicated, simulated robots to walk, like the 'Atlas' model from  Boston Dynamics shown below; the model has 30 distinct joints, versus 17 for the bipedal robot. Other researchers have used PPO to train simulated robots to perform impressive feats of parkour while running over obstacles.This release of baselines includes scalable, parallel implementations of PPO and TRPO which both use MPI for data passing. Both use Python3 and TensorFlow. We're also adding pre-trained versions of the policies used to train the above robots to the Roboschool agent zoo.Update: We're also releasing a GPU-enabled implementation of PPO, called PPO2. This runs approximately 3X faster than the current PPO baseline on Atari. In addition, we're releasing an implementation of Actor Critic with Experience Replay (ACER),  a sample-efficient policy gradient algorithm. ACER makes use of a replay buffer, enabling it to perform more than one gradient update using each piece of sampled experience, as well as a Q-Function approximate trained with the Retrace algorithm.We re looking for people to help build and optimize our reinforcement learning algorithm codebase. If you re excited about RL, benchmarking, thorough experimentation, and open source, please apply, and mention that you read the baselines PPO post in your application.\",\n",
       " '\\n        Stay up to date with OpenAI by subscribing to our newsletter. We send one every month or so.\\n      ',\n",
       " \"We've created images that reliably fool neural network classifiers when viewed from varied scales and perspectives. This challenges a claim from last week that self-driving cars would be hard to trick maliciously since they capture images from multiple scales, angles, perspectives, and the like.Out-of-the-box adversarial examples do fail under image transformations. Below, we show the same cat picture, adversarially perturbed to be incorrectly classified as a desktop computer by Inception v3 trained on ImageNet. A zoom of as little as 1.002 causes the classification probability for the correct label tabby cat to override the adversarial label desktop computer.However, we'd suspected that active effort could produce a robust adversarial example, as adversarial examples have been shown to transfer to the physical world.Adversarial examples can be created using an optimization method called projected gradient descent to find small perturbations to the image that arbitrarily fool the classifier.Instead of optimizing for finding an input that's adversarial from a single viewpoint, we optimize over a large ensemble of stochastic classifiers that randomly rescale the input before classifying it. Optimizing against such an ensemble produces robust adversarial examples that are scale-invariant.Even when we restrict ourselves to only modifying pixels corresponding to the cat, we can create a single perturbed image that is simultaneously adversarial at all desired scales.By adding random rotations, translations, scales, noise, and mean shifts to our training perturbations, the same technique produces a single input that remains adversarial under any of these transformations.Our transformations are sampled randomly at test time, demonstrating that our example is invariant to the whole distribution of transformations.\",\n",
       " \"We're open-sourcing a high-performance Python library for robotic simulation using the MuJoCo engine, developed over our past year of robotics research.This library is one of our core tools for deep learning robotics research, which we've now released as a major version of mujoco-py, our Python 3 bindings for MuJoCo. mujoco-py 1.50.1.0 brings a number of new capabilities and significant performance boosts. New features include:Batched SimulationMany methods in trajectory optimization and reinforcement learning (like LQR, PI2, and TRPO) benefit from being able to run multiple simulations in parallel. mujoco-py uses data parallelism through OpenMP and direct-access memory management through Cython and NumPy to make batched simulation more efficient.Naive usage of the new version s MjSimPool interface shows a 400% speedup over the old, and still about 180% over an optimized and restricted usage pattern using Python s multiprocessing package to gain the same level of parallelism. The majority of the speedup comes from reduced access times to the various MuJoCo data structures. Check out examples/simpool.py for a tour of MjSimPool.High Performance Texture Randomization\\n\\nWe use the domain randomization technique across many projects at OpenAI. The latest version of mujoco-py supports headless GPU rendering; this yields a speedup of ~40x compared to CPU-based rendering, letting us generate hundreds of frames per second of synthetic image data. In the above (slowed down) animation we use this to vary the textures of one of our robots, which helps it identify its body when we transfer it from the simulator to reality. Check out examples/disco_fetch.py for an example of randomized texture generation.Virtual Reality with mujoco-pyThe API exposed by mujoco-py is sufficient to enable Virtual Reality interaction without any extra C++ code. We ported MuJoCo s C++ VR example to Python using mujoco-py. If you have an HTC Vive VR setup, you can give try it using this example (this support is considered experimental, but we've been using it internally for a while).API and UsageThe simplest way to get started with mujoco-py is with the MjSim class. It is a wrapper around the simulation model and data, and lets you to easily step the simulation and render images from camera sensors. Here's a simple example:For advanced users, we provide a number of lower-level interfaces for accessing the innards of the MuJoCo C structs and functions directly. Refer to the README and the full documentation to learn more.\",\n",
       " \"One step towards building safe AI systems is to remove the need for humans to write goal functions, since using a simple proxy for a complex goal, or getting the complex goal a bit wrong, can lead to undesirable and even dangerous behavior. In collaboration with DeepMind's safety team, we've developed an algorithm which can infer what humans want by being told which of two proposed behaviors is better.We present a learning algorithm that uses small amounts of human feedback to solve modern RL environments. Machine learning systems with human feedback have been explored before, but we've scaled up the approach to be able to work on much more complicated tasks. Our algorithm needed 900 bits of feedback from a human evaluator to learn to backflip   a seemingly simple task which is simple to judge but challenging to specify.\\nOur algorithm learned to backflip using around 900 individual bits of feedback from the human evaluator.The overall training process is a 3-step feedback cycle between the human, the agent s understanding of the goal, and the RL training.Our AI agent starts by acting randomly in the environment. Periodically, two video clips of its behavior are given to a human, and the human decides which of the two clips is closest to fulfilling its goal   in this case, a backflip. The AI gradually builds a model of the goal of the task by finding the reward function that best explains the human s judgments. It then uses RL to learn how to achieve that goal. As its behavior improves, it continues to ask for human feedback on trajectory pairs where it's most uncertain about which is better, and further refines its understanding of the goal.Our approach demonstrates promising sample efficiency   as stated previously, the backflip video required under 1000 bits of human feedback. It took less than an hour of a human evaluator's time, while in the background the policy accumulated about 70 hours of overall experience (simulated at a much faster rate than real-time.) We will continue to work on reducing the amount of feedback a human needs to supply. You can see a sped-up version of the training process in the following video.We've tested our method on a number of tasks in the simulated robotics and Atari domains (without being given access to the reward function: so in Atari, without having access to the game score). Our agents can learn from human feedback to achieve strong and sometimes superhuman performance in many of the environments we tested. In the following animation you can see agents trained with our technique playing a variety of Atari games. The horizontal bar on the right hand side of each frame represent's each agents prediction about how much a human evaluator would approve of their current behavior. These visualizations indicate that agents trained with human feedback learn to value oxygen in Seaquest (left), anticipate rewards in Breakout and Pong (center), or work out how to recover from crashes in Enduro (right).Note there's no need for the feedback to align with the environment's normal reward function: we can, for example, train our agents to precisely keep even with other cars in Enduro rather than maximizing game score by passing them. We also sometimes find that learning from feedback does better than reinforcement learning with the normal reward function, because the human shapes the reward better than whoever wrote the environment's reward.Our algorithm's performance is only as good as the human evaluator's intuition about what behaviors look correct, so if the human doesn't have a good grasp of the task they may not offer as much helpful feedback. Relatedly, in some domains our system can result in agents adopting policies that trick the evaluators. For example, a robot which was supposed to grasp items instead positioned its manipulator in between the camera and the object so that it only appeared to be grasping it, as shown below.We addressed this particular problem by adding in visual cues (the thick white lines in the above animation) to make it easy for the human evaluators to estimate depth.The research described in this post was done in collaboration with Jan Leike, Miljan Martic, and Shane Legg at DeepMind. Our two organizations plan to continue to collaborate on topics that touch on long-term AI safety. We think that techniques like this are a step towards safe AI systems capable of learning human-centric goals, and can complement and extend existing approaches like reinforcement and imitation learning. This post is representative of the work done by OpenAI's safety team; if you're interested in working on problems like this, please join us!Footnote:By comparison, we took two hours to write our own reward function (the animation in the above right) to get a robot to backflip, and though it succeeds it's a lot less elegant than the one trained simply through human feedback (top left). We think there are many cases where human feedback could let us specify a specific goal more intuitively and quickly than is possible by manually hand-crafting the objective.You can replicate this backflip in gym with the following reward function for Hopper:\",\n",
       " 'Multiagent environments where agents compete for resources are stepping stones on the path to AGI. Multiagent environments have two useful properties: first, there is a natural curriculum   the difficulty of the environment is determined by the skill of your competitors (and if you\\'re competing against clones of yourself, the environment exactly matches your skill level). Second, a multiagent environment has no stable equilibrium: no matter how smart an agent is, there\\'s always pressure to get smarter. These environments have a very different feel from traditional environments, and it\\'ll take a lot more research before we become good at them.We\\'ve developed a new algorithm, MADDPG, for centralized learning and decentralized execution in multiagent environments, allowing agents to learn to collaborate and compete with each other.MADDPG used to train four red agents to chase two green agents. The red agents have learned to team up with one another to chase a single green agent, gaining higher reward. The green agents, meanwhile, learned to split up, and while one is being chased the other tries to approach the water (blue circle) while avoiding the red agents.MADDPG extends a reinforcement learning algorithm called DDPG, taking inspiration from actor-critic reinforcement learning techniques; other groups are exploring variations and parallel implementations of these ideas.We treat each agent in our simulation as an \"actor\", and each actor gets advice from a \"critic\" that helps the actor decide what actions to reinforce during training. Traditionally, the critic tries to predict the value (i.e. the reward we expect to get in the future) of an action in a particular state, which is used by the agent - the actor - to update its policy. This is more stable than directly using the reward, which can vary considerably. To make it feasible to train multiple agents that can act in a globally-coordinated way, we enhance our critics so they can access the observations and actions of all the agents, as the following diagram shows.Our agents don\\'t need to access the central critic at test time; they act based on their observations in combination with their predictions of other agents behaviors\\'. Since a centralized critic is learned independently for each agent, our approach can also be used to model arbitrary reward structures between agents, including adversarial cases where the rewards are opposing.We tested our approach on a variety of tasks and it performed better than DDPG on all of them. In the above animations you can see, from left to right: two AI agents trying to go to a specific location and learning to split up to hide their intended location from the opposing agent; one agent communicating the name of a landmark to another agent; and three agents coordinating to travel to landmarks without bumping into each other.Red agents trained with MADDPG exhibit more complex behaviors than those trained with DDPG. In the above animation we see agents trained with our technique (left) and DDPG (right) attempting to chase green agents through green forests and around black obstacles. Our agents catch more agents and visibly coordinate more than those trained with DDPG.Where Traditional RL StrugglesTraditional decentralized RL approaches   DDPG, actor-critic learning, deep Q-learning, and so on   struggle to learn in multiagent environments, as at every time step each agent will be trying to learn to predict the actions of other agents while also taking its own actions. This is especially true in competitive situations. MADDPG employs a centralized critic to supply agents with information about their peers\\' observations and potential actions, transforming an unpredictable environment into a predictable one.Using policy gradient methods presents further challenges: because these exhibit high variance learning the right policy is difficult to do when the reward is inconsistent. We also found that adding in a critic, while improving stability, still failed to solve several of our environments such as cooperative communication. It seems that considering the actions of others during training is important for learning collaborative strategies.Initial ResearchBefore we developed MADDPG, when using decentralized techniques, we noticed that listener agents would often learn to ignore the speaker if it sent inconsistent messages about where to go to. The agent would then set all the weights associated with the speaker s message to 0, effectively deafening itself. Once this happens, it s hard for training to recover, since the speaker will never know if it says the right thing due to the absence of any feedback. To fix this, we looked at a technique outlined in a recent hierarchical reinforcement project, which lets us force the listener to incorporate the utterances of the speaker in its decision-making process. This fix didn t work, because though it forces the listener to pay attention to the speaker, it doesn\\'t help the speaker figure out what to say that is relevant. Our centralized critic method helps deal with these challenges, by helping the speaker to learn which utterances might be relevant to the actions of other agents. For more of our results, you can watch the following video:Next StepsAgent modeling has a rich history within artificial intelligence research and many of these scenarios have been studied before. Lots of previous research considered games with only a small number of time steps with a small state space. Deep learning lets us deal with complex visual inputs and RL gives us tools to learn behaviors over long time periods. Now that we can use these capabilities to train multiple-agents at once without them needing to know the dynamics of the environment (how the environment changes at each time-step), we can tackle a wider range of problems involving communication and language while learning from environments\\' high-dimensional information. If you re interesting in exploring different approaches to evolving agents then consider joining OpenAI.',\n",
       " 'We\\'re open-sourcing OpenAI Baselines, our internal effort to reproduce reinforcement learning algorithms with performance on par with published results. We\\'ll release the algorithms over upcoming months; today\\'s release includes DQN and three of its variants.Reinforcement learning results are tricky to reproduce: performance is very noisy, algorithms have many moving parts which allow for subtle bugs, and many papers don\\'t report all the required tricks. By releasing known-good implementations (and best practices for creating them), we d like to ensure that apparent RL advances never are due to comparison with buggy or untuned versions of existing algorithms.\\nThis post contains some best practices we use for correct RL algorithm implementations, as well as the details of our first release: DQN and three of its variants, algorithms developed by DeepMind.Best PracticesCompare to a random baseline: in the video below, an agent is taking random actions in the game H.E.R.O. If you saw this behavior in early stages of training, it\\'d be really easy to trick yourself into believing that the agent is learning. So you should always verify your agent outperforms a random one.Be wary of non-breaking bugs: when we looked through a sample of ten popular reinforcement learning algorithm reimplementations we noticed that six had subtle bugs found by a community member and confirmed by the author. These ranged from mild bugs that ignored gradients on some examples or implemented causal convolutions incorrectly to serious ones that reported scores higher than the true result.See the world as your agent does: like most deep learning approaches, for DQN we tend to convert images of our environments to grayscale to reduce the computation required during training. This can create its own bugs: when we ran our DQN algorithm on Seaquest we noticed that our implementation was performing poorly. When we inspected the environment we discovered this was because our post-processed images contained no fish, as this picture shows.When transforming the screen images into greyscale we had incorrectly calibrated our coefficients for the green color values, which led to the fish disappearing. After we noticed the bug we tweaked the color values and our algorithm was able to see the fish again.To debug issues like this in the future, Gym now contains a play function, which lets a researcher easily see the same observations as the AI agent would.Fix bugs, then hyperparameters: After debugging, we started to calibrate our hyperparameters. We ultimately found that setting the annealing schedule for epsilon, a hyperparameter which controlled the exploration rate, had a huge impact on performance. Our final implementation decreases epsilon to 0.1 over the first million steps and then down to 0.01 over the next 24 million steps. If our implementation contained bugs, then it\\'s likely we would come up with different hyperparameter settings to try to deal with faults we hadn\\'t yet diagnosed.Double check your interpretations of papers: In the DQN Nature paper the authors write: \"We also found it helpful to clip the error term from the update [...] to be between -1 and 1.\". There are two ways to interpret this statement   clip the objective, or clip the multiplicative term when computing gradient. The former seems more natural, but it causes the gradient to be zero on transitions with high error, which leads to suboptimal performance, as found in one DQN implementation. The latter is correct and has a simple mathematical interpretation   Huber Loss. You can spot bugs like these by checking that the gradients appear as you expect   this can be easily done  within TensorFlow by using \\ncompute_gradients.The majority of bugs in this post were spotted by going over the code multiple times and thinking through what could go wrong with each line. Each bug seems obvious in hindsight, but even experienced researchers tend to underestimate how many passes over the code it can take to find all the bugs in an implementation.Deep Q-LearningWe use Python 3 and TensorFlow. This release includes:To get started, run the following:We\\'ve also provided trained agents, which you can obtain by running:BenchmarksWe\\'ve included an iPython notebook showing the performance of our DQN implementations on Atari games. You can compare the performance of our various algorithms such as Dueling Double Q learning with Prioritized Replay (yellow), Double Q learning with Prioritized Replay (blue), Dueling Double Q learning (green) and Double Q learning (red).AI is an empirical science, where the ability to do more experiments directly correlates with progress. With Baselines, researchers can spend less time implementing pre-existing algorithms and more time designing new ones. If you\\'d like to help us refine, extend, and develop AI algorithms then join us at OpenAI.',\n",
       " \"We've created a robotics system, trained entirely in simulation and deployed on a physical robot, which can learn a new task after seeing it done once.Last month, we showed an earlier version of this robot where we'd trained its vision system using domain randomization, that is, by showing it simulated objects with a variety of color, backgrounds, and textures, without the use of any real images.Now, we've developed and deployed a new algorithm, one-shot imitation learning, allowing a human to communicate how to do a new task by performing it in VR. Given a single demonstration, the robot is able to solve the same task from an arbitrary starting configuration.\\nCaption: Our system  can learn a behavior from a single demonstration delivered within a simulator, then reproduce that behavior in different setups in reality.The system is powered by two neural networks: a vision network and an imitation network.The vision network ingests an image from the robot's camera and outputs state representing the positions of the objects. As before, the vision network is trained with hundreds of thousands of simulated images with different perturbations of lighting, textures, and objects. (The vision system is never trained on a real image.)The imitation network observes a demonstration, processes it to infer the intent of the task, and then accomplishes the intent starting from another starting configuration. Thus, the imitation network must generalize the demonstration to a new setting. But how does the imitation network know how to generalize The network learns this from the distribution of training examples. It is trained on dozens of different tasks with thousands of demonstrations for each task. Each training example is a pair of demonstrations that perform the same task. The network is given the entirety of the first demonstration and a single observation from the second demonstration. We then use supervised learning to predict what action the demonstrator took at that observation. In order to predict the action effectively, the robot must learn how to infer the relevant portion of the task from the first demonstration.Applied to block stacking, the training data consists of pairs of trajectories that stack blocks into a matching set of towers in the same order, but start from different start states. In this way, the imitation network learns to match the demonstrator's ordering of blocks and size of towers without worrying about the relative location of the towers.The task of creating color-coded stacks of blocks is simple enough that we were able to solve it with a scripted policy in simulation. We used the scripted policy to generate the training data for the imitation network. At test time, the imitation network was able to parse demonstrations produced by a human, even though it had never seen messy human data before.The imitation network uses soft attention over the demonstration trajectory and the state vector which represents the locations of the blocks, allowing the system to work with demonstrations of variable length. It also performs attention over the locations of the different blocks, allowing it to imitate longer trajectories than it's ever seen, and stack blocks into a configuration that has more blocks than any demonstration in its training data.For the imitation network to learn a robust policy, we had to inject a modest amount of noise into the outputs of the scripted policy.  This forced the scripted policy to demonstrate how to recover when things go wrong, which taught the imitation network to deal with the disturbances from an imperfect policy. Without injecting the noise, the policy learned by the imitation network would usually fail to complete the stacking task.If you d like to help us build this robot, join us at OpenAI.\",\n",
       " \"We are releasing Roboschool: open-source software for robot simulation, integrated with OpenAI Gym.Roboschool provides new OpenAI Gym environments for controlling robots in simulation. Eight of these environments serve as free alternatives to pre-existing MuJoCo implementations, re-tuned to produce more realistic motion. We also include several new, challenging environments.Roboschool also makes it easy to train multiple agents together in the same environment.After we launched Gym, one issue we heard from many users was that the MuJoCo component required a paid license (though MuJoCo recently added free student licenses for personal and class work). Roboschool removes this constraint, letting everyone conduct research regardless of their budget. Roboschool is based on the Bullet Physics Engine, an open-source, permissively licensed physics library that has been used by other simulation software such as Gazebo and V-REP.Roboschool ships with twelve environments, including tasks familiar to Mujoco users as well as new challenges, such as harder versions of the Humanoid walker task, and a multi-player Pong environment. We plan to expand this collection over time and look forward to the community contributing as well.For the existing MuJoCo environments, besides porting them to Bullet, we have modified them to be more realistic. Here are three of the environments we ported, with explanations of how they differ from the existing environments.You can find trained policies for all of these environments in the agent_zoo folder in the GitHub repository. You can also access a demo_race script to initiate a race between three robots.In several of the previous OpenAI Gym environments, the goal was to learn a walking controller. However, these environments involved a very basic version of the problem, where the goal is simply to move forward. In practice, the walking policies would learn a single cyclic trajectory and leave most of the state space unvisited. Furthermore, the final policies tended to be very fragile: a small push would often cause the robot to crash and fall.We have added two more environments with the 3D humanoid, which make the locomotion problem more interesting and challenging. These environments require interactive control   the robots must run towards a flag, whose position randomly varies over time.\\n\\n\\nYour browser does not support the video tag.\\n\\nHumanoidFlagrun is designed to teach the robot to slow down and turn. The goal is to run towards the flag, whose position varies randomly.HumanoidFlagrunHarder in addition allows the robot to fall and gives it time to get back on foot. It also starts each episode upright or laying on the ground, and the robot is constantly bombarded by white cubes to push it off its trajectory.We ship trained policies for both HumanoidFlagrun and HumanoidFlagrunHarder. The walks aren't as fast and natural-looking as the ones we see from the regular humanoid, but these policies can recover from many situations, and they know how to steer. This policy itself is still a multilayer perceptron, which has no internal state, so we believe that in some cases the agent uses its arms to store information.Roboschool lets you both run and train multiple agents in the same environment. We start with RoboschoolPong, with more environments to follow.With multiplayer training, you can train the same agent playing for both parties (so it plays with itself), you can train two different agents using the same algorithm, or you can even set two different algorithms against each other.The multi-agent setting presents some interesting challenges. If you train both players simultaneously, you ll likely see a learning curve like the following one, obtained from a policy gradient method:Here s what s happening:That way, the policies oscillated, and neither agent learned anything useful after hours of training. As in generative adversarial networks, learning in an adversarial setting is tricky, but we think it s an interesting research problem because this interplay can lead to sophisticated strategies even in simple environments, and it can provide a natural curriculum.There s been a lot of work by the community to create environments for OpenAI Gym, some of which are based on open-source physics simulators. In one recent project, researchers created a fork of OpenAI Gym that replaced MuJoCo by the open-source physics simulator DART. They showed that policies can even be transferred between the two physics simulators, MuJoCo and DART.\",\n",
       " 'We ve developed an unsupervised system which learns an excellent representation of sentiment, despite being trained only to predict the next character in the text of Amazon reviews.A linear model using this representation achieves state-of-the-art sentiment analysis accuracy on a small but extensively-studied dataset, the Stanford Sentiment Treebank (we get 91.8% accuracy versus the previous best of 90.2%), and can match the performance of previous supervised systems using 30-100x fewer labeled examples. Our representation also contains a distinct  sentiment neuron  which contains almost all of the sentiment signal.Our system beats other approaches on Stanford Sentiment Treebank while using dramatically less data.\\nThe number of labeled examples it takes two variants of our model (the green and blue lines) to match fully supervised approaches, each trained with 6,920 examples (the dashed gray lines). Our L1-regularized model (pretrained in an unsupervised fashion on Amazon reviews) matches multichannel CNN performance with only 11 labeled examples, and state-of-the-art CT-LSTM Ensembles with 232 examples.We were very surprised that our model learned an interpretable feature, and that simply predicting the next character in Amazon reviews resulted in discovering the concept of sentiment. We believe the phenomenon is not specific to our model, but is instead a general property of certain large neural networks that are trained to predict the next step or dimension in their inputs.We first trained a multiplicative LSTM with 4,096 units on a corpus of 82 million Amazon reviews to predict the next character in a chunk of text. Training took one month across four NVIDIA Pascal GPUs, with our model processing 12,500 characters per second.These 4,096 units (which are just a vector of floats) can be regarded as a feature vector representing the string read by the model. After training the mLSTM, we turned the model into a sentiment classifier by taking a linear combination of these units, learning the weights of the combination via the available supervised data.While training the linear model with L1 regularization, we noticed it used surprisingly few of the learned units. Digging in, we realized there actually existed a single  sentiment neuron  that\\'s highly predictive of the sentiment value.\\nThe sentiment neuron within our model can classify reviews as negative or positive, even though the model is trained only to predict the next character in the text.Just like with similar models, our model can be used to generate text. Unlike those models, we have a direct dial to control the sentiment of the resulting text: we simply overwrite the value of the sentiment neuron.Examples of synthetic text generated by the trained model. Above, we select random samples from the model after fixing the sentiment unit\\'s value to determine the sentiment of the review. Below, we also pass the prefix \"I couldn\\'t figure out\" through the model and select high-likelihood samples only.The diagram below represents the character-by-character value of the sentiment neuron, displaying negative values as red and positive values as green. Note that strongly indicative words like  best  or  horrendous  cause particularly big shifts in the color.\\nThe sentiment neuron adjusting its value on a character-by-character basis.It\\'s interesting to note that the system also makes large updates after the completion of sentences and phrases. For example, in  And about 99.8 percent of that got lost in the film , there s a negative update after  lost  and a larger update at the sentence s end, even though  in the film  has no sentiment content on its own.Labeled data are the fuel for today\\'s machine learning. Collecting data is easy, but scalably labeling that data is hard. It s only feasible to generate labels for important problems where the reward is worth the effort, like machine translation, speech recognition, or self-driving.Machine learning researchers have long dreamed of developing unsupervised learning algorithms to learn a good representation of a dataset, which can then be used to solve tasks using only a few labeled examples. Our research implies that simply training large unsupervised next-step-prediction models on large amounts of data may be a good approach to use when creating systems with good representation learning capabilities.Our results are a promising step towards general unsupervised representation learning. We found the results by exploring whether we could learn good quality representations as a side effect of language modeling, and scaled up an existing model on a carefully-chosen dataset. Yet the underlying phenomena remain more mysterious than clear.Overall, it\\'s important to understand the properties of models, training regimes, and datasets that reliably lead to such excellent representations.',\n",
       " 'We\\'ve created the world\\'s first Spam-detecting AI trained entirely in simulation and deployed on a physical robot.\\nOur vision system successfully flagging a can of Spam for removal. The vision system is trained entirely in simulation, while the movement policy for grasping and removing the Spam is hard-coded. Our detector is able to avoid other objects, including healthy ones such as fruit and vegetables, which it never saw during training.Deep learning-driven robotic systems are bottlenecked by data collection: it\\'s extremely costly to obtain the hundreds of thousands of images needed to train the perception system alone. It\\'s cheap to generate simulated data, but simulations diverge enough from reality that people typically retrain models from scratch when moving to the physical world.We\\'ve shown that domain randomization, an existing idea for making detectors trained on simulated images transfer to real images, works well for cluttered scenes. The method is simple: we randomly vary colors, textures, lighting conditions, and camera settings in simulated scenes. The resulting dataset is sufficiently variable to allow a deep neural network trained on it to generalize to reality.\\nRandomly generated scenes. Each frame contains Spam, often hidden among distractor objects. Our Spam model is sourced from the YCB dataset.The detector is a neural network based on the VGG16 architecture that predicts the precise 3-D location of Spam in simulated images. Though it has only been trained on simulated scenes, the resulting network is able to detect Spam in real images, even in the presence of never-before-seen \"distractor\" items arranged in random configurations.The video below demonstrates the system in action:In the future, we plan to extend this work to detect phishing and to defend against adversarial Spam.If you\\'d like to sink your teeth into compelling applied research problems like Spam detection, consider joining us at OpenAI.',\n",
       " 'We\\'ve discovered that evolution strategies (ES), an optimization technique that\\'s been known for decades, rivals the performance of standard reinforcement learning (RL) techniques on modern RL benchmarks (e.g. Atari/MuJoCo), while overcoming many of RL\\'s inconveniences.In particular, ES is simpler to implement (there is no need for backpropagation), it is easier to scale in a distributed setting, it does not suffer in settings with sparse rewards, and has fewer hyperparameters. This outcome is surprising because ES resembles simple hill-climbing in a high-dimensional space based only on finite differences along a few random directions at each step.Our finding continues the modern trend of achieving strong results with decades-old ideas. For example, in 2012, the \"AlexNet\" paper showed how to design, scale and train convolutional neural networks (CNNs) to achieve extremely strong results on image recognition tasks, at a time when most researchers thought that CNNs were not a promising approach to computer vision. Similarly, in 2013, the Deep Q-Learning paper showed how to combine Q-Learning with CNNs to successfully solve Atari games, reinvigorating RL as a research field with exciting experimental (rather than theoretical) results. Likewise, our work demonstrates that ES achieves strong performance on RL benchmarks, dispelling the common belief that ES methods are impossible to apply to high dimensional problems.ES is easy to implement and scale. Running on a computing cluster of 80 machines and 1,440 CPU cores, our implementation is able to train a 3D MuJoCo humanoid walker in only 10 minutes (A3C on 32 cores takes about 10 hours). Using 720 cores we can also obtain comparable performance to A3C on Atari while cutting down the training time from 1 day to 1 hour.In what follows, we\\'ll first briefly describe the conventional RL approach, contrast that with our ES approach, discuss the tradeoffs between ES and RL, and finally highlight some of our experiments.Let\\'s briefly look at how RL works. Suppose we are given some environment (e.g. a game) that we\\'d like to train an agent on. To describe the behavior of the agent, we define a policy function (the brain of the agent), which computes how the agent should act in any given situation. In practice, the policy is usually a neural network that takes the current state of the game as an input and calculates the probability of taking any of the allowed actions. A typical policy function might have about 1,000,000 parameters, so our task comes down to finding the precise setting of these parameters such that the policy plays well (i.e. wins a lot of games).Above: In the game of Pong, the policy could take the pixels of the screen and compute the probability of moving the player\\'s paddle (in green, on right) Up, Down, or neither.The training process for the policy works as follows. Starting from a random initialization, we let the agent interact with the environment for a while and collect episodes of interaction (e.g. each episode is one game of Pong). We thus obtain a complete recording of what happened: what sequence of states we encountered, what actions we took in each state, and what the reward was at each step. As an example, below is a diagram of three episodes that each took 10 time steps in a hypothetical environment. Each rectangle is a state, and rectangles are colored green if the reward was positive (e.g. we just got the ball past our opponent) and red if the reward was negative (e.g. we missed the ball):This diagram suggests a recipe for how we can improve the policy; whatever we happened to do leading up to the green states was good, and whatever we happened to do in the states leading up to the red areas was bad. We can then use backpropagation to compute a small update on the network\\'s parameters that would make the green actions more likely in those states in the future, and the red actions less likely in those states in the future. We expect that the updated policy works a bit better as a result. We then iterate the process: collect another batch of episodes, do another update, etc.Exploration by injecting noise in the actions. The policies we usually use in RL are stochastic, in that they only compute probabilities of taking any action. This way, during the course of training, the agent may find itself in a particular state many times, and at different times it will take different actions due to the sampling. This provides the signal needed for learning; some of those actions will lead to good outcomes, and get encouraged, and some of them will not work out, and get discouraged. We therefore say that we introduce exploration into the learning process by injecting noise into the agent\\'s actions, which we do by sampling from the action distribution at each time step. This will be in contrast to ES, which we describe next.On \"Evolution\". Before we dive into the ES approach, it is important to note that despite the word \"evolution\", ES has very little to do with biological evolution. Early versions of these techniques may have been inspired by biological evolution and the approach can, on an abstract level, be seen as sampling a population of individuals and allowing the successful individuals to dictate the distribution of future generations. However, the mathematical details are so heavily abstracted away from biological evolution that it is best to think of ES as simply a class of black-box stochastic optimization techniques.Black-box optimization. In ES, we forget entirely that there is an agent, an environment, that there are neural networks involved, or that interactions take place over time, etc. The whole setup is that 1,000,000 numbers (which happen to describe the parameters of the policy network) go in, 1 number comes out (the total reward), and we want to find the best setting of the 1,000,000 numbers. Mathematically, we would say that we are optimizing a function f(w) with respect to the input vector w (the parameters / weights of the network), but we make no assumptions about the structure of f, except that we can evaluate it (hence \"black box\").The ES algorithm. Intuitively, the optimization is a \"guess and check\" process, where we start with some random parameters and then repeatedly 1) tweak the guess a bit randomly, and 2) move our guess slightly towards whatever tweaks worked better. Concretely, at each step we take a parameter vector w and generate a population of, say, 100 slightly different parameter vectors w1 ... w100 by jittering w with gaussian noise. We then evaluate each one of the 100 candidates independently by running the corresponding policy network in the environment for a while, and add up all the rewards in each case. The updated parameter vector then becomes the weighted sum of the 100 vectors, where each weight is proportional to the total reward (i.e. we want the more successful candidates to have a higher weight). Mathematically, you\\'ll notice that this is also equivalent to estimating the gradient of the expected reward in the parameter space using finite differences, except we only do it along 100 random directions. Yet another way to see it is that we\\'re still doing RL (Policy Gradients, or REINFORCE specifically), where the agent\\'s actions are to emit entire parameter vectors using a gaussian policy.Above: ES optimization process, in a setting with only two parameters and a reward function (red = high, blue = low). At each iteration we show the current parameter value (in white), a population of jittered samples (in black), and the estimated gradient (white arrow). We keep moving the parameters to the top of the arrow until we converge to a local optimum. You can reproduce this figure with this notebook.Code sample. To make the core algorithm concrete and to highlight its simplicity, here is a short example of optimizing a quadratic function using ES (or see this longer version with more comments):Injecting noise in the parameters. Notice that the objective is identical to the one that RL optimizes: the expected reward. However, RL injects noise in the action space and uses backpropagation to compute the parameter updates, while ES injects noise directly in the parameter space. Another way to describe this is that RL is a \"guess and check\" on actions, while ES is a \"guess and check\" on parameters. Since we\\'re injecting noise in the parameters, it is possible to use deterministic policies (and we do, in our experiments). It is also possible to add noise in both actions and parameters to potentially combine the two approaches.ES enjoys multiple advantages over RL algorithms (some of them are a little technical):Conversely, we also found some challenges to applying ES in practice. One core problem is that in order for ES to work, adding noise in parameters must lead to different outcomes to obtain some gradient signal. As we elaborate on in our paper, we found that the use of virtual batchnorm can help alleviate this problem, but further work on effectively parameterizing neural networks to have variable behaviors as a function of noise is necessary. As an example of a related difficulty, we found that in Montezuma\\'s Revenge, one is very unlikely to get the key in the first level with a random network, while this is occasionally possible with random actions.We compared the performance of ES and RL on two standard RL benchmarks: MuJoCo control tasks and Atari game playing. Each MuJoCo task (see examples below) contains a physically-simulated articulated figure, where the policy receives the positions of all joints and has to output the torques to apply at each joint in order to move forward. Below are some example agents trained on three MuJoCo control tasks, where the objective is to move forward:We usually compare the performance of algorithms by looking at their efficiency of learning from data; as a function of how many states we\\'ve seen, what is our average reward  Here are the example learning curves that we obtain, in comparison to RL (the TRPO algorithm in this case):Data efficiency comparison. The comparisons above show that ES (orange) can reach a comparable performance to TRPO (blue), although it doesn\\'t quite match or surpass it in all cases. Moreover, by scanning horizontally we can see that ES is less efficient, but no worse than about a factor of 10 (note the x-axis is in log scale).Wall clock comparison. Instead of looking at the raw number of states seen, one can argue that the most important metric to look at is the wall clock time: how long (in number of seconds) does it take to solve a given problem  This quantity ultimately dictates the achievable speed of iteration for a researcher. Since ES requires negligible communication between workers, we were able to solve one of the hardest MuJoCo tasks (a 3D humanoid) using 1,440 CPUs across 80 machines in only 10 minutes. As a comparison, in a typical setting 32 A3C workers on one machine would solve this task in about 10 hours. It is also possible that the performance of RL could also improve with more algorithmic and engineering effort, but we found that naively scaling A3C in a standard cloud CPU setting is challenging due to high communication bandwidth requirements.Below are a few videos of 3D humanoid walkers trained with ES. As we can see, the results have quite a bit of variety, based on which local minimum the optimization ends up converging into.On Atari, ES trained on 720 cores in 1 hour achieves comparable performance to A3C trained on 32 cores in 1 day. Below are some result snippets on Pong, Seaquest and Beamrider. These videos show the preprocessed frames, which is exactly what the agent sees when it is playing:In particular, note that the submarine in Seaquest correctly learns to go up when its oxygen reaches low levels.ES is an algorithm from the neuroevolution literature, which has a long history in AI and a complete literature review is beyond the scope of this post. However, we encourage an interested reader to look at Wikipedia, Scholarpedia, and J rgen Schmidhuber\\'s review article (Section 6.6). The work that most closely informed our approach is Natural Evolution Strategies by Wierstra et al. 2014. Compared to this work and much of the work it has inspired, our focus is specifically on scaling these algorithms to large-scale, distributed settings, finding components that make the algorithms work better with deep neural networks (e.g. virtual batch norm), and evaluating them on modern RL benchmarks.It is also worth noting that neuroevolution-related approaches have seen some recent resurgence in the machine learning literature, for example with HyperNetworks, \"Large-Scale Evolution of Image Classifiers\" and \"Convolution by Evolution\".Our work suggests that neuroevolution approaches can be competitive with reinforcement learning methods on modern agent-environment benchmarks, while offering significant benefits related to code complexity and ease of scaling to large-scale distributed settings. We also expect that more exciting work can be done by revisiting other ideas from this line of work, such as indirect encoding methods, or evolving the network structure in addition to the parameters.Note on supervised learning. It is also important to note that supervised learning problems (e.g. image classification, speech recognition, or most other tasks in the industry), where one can compute the exact gradient of the loss function with backpropagation, are not directly impacted by these findings. For example, in our preliminary experiments we found that using ES to estimate the gradient on the MNIST digit recognition task can be as much as 1,000 times slower than using backpropagation. It is only in RL settings, where one has to estimate the gradient of the expected reward by sampling, where ES becomes competitive.Code release. Finally, if you\\'d like to try running ES yourself, we encourage you to dive into the full details by reading  our paper or looking at our code on this Github repo.',\n",
       " \"We're excited to support today's launch of Distill, a new kind of journal aimed at excellent communication of machine learning results (novel or existing).Distill is a website and set of associated tools that make it easier for people to explain machine learning concepts using modern web technologies. For example, people have already used the platform to explore the subtle settings of the t-SNE algorithm, to demystify the checkerboard artifacts in synthetic images, and peek under the hood of recurrent neural networks that generate handwriting.Andrej will serve on the steering committee for the publication, and Greg is helping fund the Distill Prize for Clarity in Machine Learning, which recognizes outstanding work on communicating ideas in machine learning and related topics (published in any venue!)\",\n",
       " 'In this post we\\'ll outline new OpenAI research in which agents develop their own language.Our hypothesis is that true language understanding will come from agents that learn words in combination with how they affect the world, rather than spotting patterns in a huge corpus of text. As a first step, we wanted to see if cooperative agents could develop a simple language amongst themselves.We\\'ve just released initial results in which we teach AI agents to create language by dropping them into a set of simple worlds, giving them the ability to communicate, and then giving them goals that can be best achieved by communicating with other agents. If they achieve a goal, then they get rewarded. We train them using reinforcement learning and, due to careful experiment design, they develop a shared language to help them achieve their goals.Our approach yields agents that invent a (simple!) language which is grounded and compositional. Grounded means that words in a language are tied to something directly experienced by a speaker in their environment, for example, a speaker forming an association between the word \"tree\" and images or experiences of trees. Compositional means that speakers can assemble multiple words into a sentence to represent a specific idea, such as getting another agent to go to a specific location.To train the agents, we represent the experiment as a cooperative   rather than competitive   multi-agent reinforcement learning problem. The agents exist in a two-dimensional world with simple landmarks, and each agent has a goal. Goals can vary from looking at or moving to a specific location, to encouraging a separate agent to move to a location. Each agent can broadcast messages to the group. Every agent\\'s reward is the sum of the rewards paid out to all agents, encouraging collaboration.At each time step, our RL agents can take two kinds of actions   (i) environment actions, like moving around or looking at things, and (ii) communication actions, like broadcasting a word to all other agents. (Note that though the agents come up with words that we found to correspond to objects and other agents, as well as actions like \\'Look at\\' or \\'Go to\\', to the agents these words are abstract symbols represented by one-hot vector   we label these one-hot vectors with English words that capture their meaning for the sake of interpretability.) Before an agent takes an action, it observes the communications from other agents from the previous time step as well as the locations of all entities and objects in the world. It stores that communication in a private recurrent neural network, giving it a memory for the words it hears.We use discrete communication actions (messages formed of separate, word-like symbols) sent over a differentiable communication channel. A communication channel is differentiable if it allows agents to directly inform each other about what message they should have sent at each time step, by slightly altering their messages to make a positive change in the reward both agents expect to receive. Agents accomplish this by calculating the gradient of future reward with respect to changes in the sent messages (i.e. how much rewards would change with different messages). For example, if one agent realizes that it could have performed a task better if a second agent had sent different information, the first agent can tell the second exactly how to modify its messages to make them as useful as possible. In other words, agents ask the question: \\'how should I modify my communication output to get the most communal reward in the future \\'.Previous efforts achieved this sort of differentiable communication by having the agents send a vector of real numbers or a continuous approximation to binary values to each other, or used non-differentiable communication and training. We use the Gumbel-Softmax trick, to approximate discrete communication decisions with a continuous representation during training. This gets us the best of both worlds: during training the differentiable channel means agents can rapidly learn how to communicate with each other via using continuous representation, which at the end of training ends up converging on discrete outputs that are more interpretable and show traits like compositionality.In the video that follows, we show how our agents evolve languages to fit the complexity of their situation, with solitary agents not needing to communicate, two agents inventing one-word phrases to coordinate with each other in simple tasks, and three agents composing multiple words in sentences to accomplish more challenging tasks.All research projects have complications; in this case, our agents frequently invented languages that didn\\'t display the compositional traits we wanted. And even when they succeeded, their solutions had their own idiosyncrasies.The first problem we ran into was the agents\\' tendency to create a single utterance and intersperse it with spaces to create meaning. This Morse code language was hard to decipher and non-compositional. To correct this, we imposed a slight cost on every utterance and added a preference for achieving the task quickly. This encouraged the agents to use their communication channel concisely, which led to the development of a larger vocabulary.Another issue we faced was agents trying to use single words to encode the meaning of entire sentences. This happened when we gave them the ability to use large vocabularies; they\\'d eventually create a single utterance that encoded the meaning of an entire sentence such as  red agent, go to blue landmark . While useful for the agents, this approach requires vocabulary size to grow exponentially with the sentence length and doesn\\'t fit with our broader goal of creating AI that is interpretable to humans.) To deter agents from creating this sort of language we incorporated a preference for compact vocabulary sizes through a preference for using already-popular words, inspired by ideas outlined in The evolution of syntactic communication. We incorporate this by putting a reward for speaking a particular word that is proportional to how frequently that word has been spoken previously.Lastly, we encountered agents inventing landmark references not based on color, but other cues such as spatial relationships. For example, agents would invent words like \"top-most\" or \"left-most\" landmark to refer to locations based on a global 2D coordinate system. While such behavior is very inventive, it is fairly specific to our particular environment implementation, and could cause problems if we substantially changed the geography of the worlds the agents live in. To fix this, we placed agents in an ego-centric coordinate frame (so that there is no single shared coordinate frame). This dealt with the odd directions, and led to them referring to landmarks by their color property.This method of training also works when agents are unable to communicate with each other via text, and have to instead carry out physical actions actions in the simulated environment. In the animations that follow we show agents improvising in this way by pointing or guiding other agents to targets, or in extreme cases pushing sightless agents to their goal.Today, many people have applied machine learning to language-related tasks with great success. Large-scale ML techniques have led to significant advances in translation, verbal reasoning, language understanding, sentence generation, and other areas. All of these approaches work by feeding them extremely large amounts of textual data, from which the systems extract features and discover patterns. While this work has yielded numerous inventions and innovations, it has drawbacks relating to the representational quality of the language that is learned. There\\'s not much indication that if you train a computer on language in this way it will have a deep understanding of how that language is attached to the real world. With our research, we\\'re trying to deal with this grounding problem by training our agents to invent language which is tied to their perception of the world.Computers whose language models are trained without grounding are much like the character trapped in John Searle s Chinese Room, where they compare incoming text against a kind of dictionary of semantic meaning which has been created through the analysis of large quantities of text. It\\'s unclear how much of an idea these computers have about what the text represents, as they\\'ve never left this room and been able to interact with the world the text describes.We hope that this research into growing a language will let us develop machines that have their own language tied to their own lived experience. We think that if we slowly increase the complexity of their environment, and the range of actions the agents themselves are allowed to take, it s possible they ll create an expressive language which contains concepts beyond the basic verbs and nouns that evolved here.As the complexity of this invented language increases, it\\'s going to become challenging for us to make these languages interpretable by humans. That\\'s why for our next project, Ryan Lowe and Igor Mordatch are going to investigate ways to connect the invented languages with English via having the agents communicate with English-speaking agents. This will automate the translation of their language into ours. This is an interdisciplinary undertaking, spanning areas of AI, linguistics, and cognitive science, and as part of it we\\'ll be collaborating with researchers at UC Berkeley. If you\\'re interested in developing smarter language models, then consider working at OpenAI.You can find out more information about the technical specifics of our research in this research paper: Emergence of Grounded Compositional Language in Multi-Agent Populations, and more about the motivations for it in: A Paradigm for Situated and Goal-Driven Language Learning.',\n",
       " 'Adversarial examples are inputs to machine learning models that an attacker has intentionally designed to cause the model to make a mistake; they\\'re like optical illusions for machines. In this post we\\'ll show how adversarial examples work across different mediums, and will discuss why securing systems against them can be difficult.At OpenAI, we think adversarial examples are a good aspect of security to work on because they represent a concrete problem in AI safety that can be addressed in the short term, and because fixing them is difficult enough that it requires a serious research effort. (Though we\\'ll need to explore many aspects of machine learning security to achieve our goal of building safe, widely distributed AI.)To get an idea of what adversarial examples look like, consider this demonstration from Explaining and Harnessing Adversarial Examples: starting with an image of a panda, the attacker adds a small perturbation that has been calculated to make the image be recognized as a gibbon with high confidence.\\nAn adversarial input, overlaid on a typical image, can cause a classifier to miscategorize a panda as a gibbon.The approach is quite robust; recent research has shown adversarial examples can be printed out on standard paper then photographed with a standard smartphone, and still fool systems.\\nAdversarial examples can be printed out on normal paper and photographed with a standard resolution smartphone and still cause a classifier to, in this case, label a \"washer\" as a \"safe\".Adversarial examples have the potential to be dangerous. For example, attackers could target autonomous vehicles by using stickers or paint to create an adversarial stop sign that the vehicle would interpret as a \\'yield\\' or other sign, as discussed in Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples.Reinforcement learning agents can also be manipulated by adversarial examples, according to new research from UC Berkeley, OpenAI, and Pennsylvania State University, Adversarial Attacks on Neural Network Policies, and research from the University of Nevada at Reno, Vulnerability of Deep Reinforcement Learning to Policy Induction Attacks. The research shows that widely-used RL algorithms, such as DQN, TRPO, and A3C, are vulnerable to adversarial inputs. These can lead to degraded performance even in the presence of pertubations too subtle to be percieved by a human, causing an agent to move a pong paddle down when it should go up, or interfering with its ability to spot enemies in Seaquest.If you want to experiment with breaking your own models, you can use cleverhans, an open source library developed jointly by Ian Goodfellow and Nicolas Papernot to test your AI\\'s vulnerabilities to adversarial examples.When we think about the study of AI safety, we usually think about some of the most difficult problems in that field   how can we ensure that sophisticated reinforcement learning agents that are significantly more intelligent than human beings behave in ways that their designers intended Adversarial examples show us that even simple modern algorithms, for both supervised and reinforcement learning, can already behave in surprising ways that we do not intend.Traditional techniques for making machine learning models more robust, such as weight decay and dropout, generally do not provide a practical defense against adversarial examples. So far, only two methods have provided a significant defense.Adversarial training: This is a brute force solution where we simply generate a lot of adversarial examples and explicitly train the model not to be fooled by each of them. An open-source implementation of adversarial training is available in the cleverhans library and its use illustrated in the following tutorial.Defensive distillation: This is a strategy where we train the model to output probabilities of different classes, rather than hard decisions about which class to output. The probabilities are supplied by an earlier model, trained on the same task using hard class labels. This creates a model whose surface is smoothed in the directions an adversary will typically try to exploit, making it difficult for them to discover adversarial input tweaks that lead to incorrect categorization. (Distillation was originally introduced in Distilling the Knowledge in a Neural Network as a technique for model compression, where a small model is trained to imitate a large one, in order to obtain computational savings.)Yet even these specialized algorithms can easily be broken by giving more computational firepower to the attacker.To give an example of how a simple defense can fail, let\\'s consider why a technique called \"gradient masking\" does not work.\"Gradient masking\" is a term introduced in Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples. to describe an entire category of failed defense methods that work by trying to deny the attacker access to a useful gradient.Most adversarial example construction techniques use the gradient of the model to make an attack. In other words, they look at a picture of an airplane, they test which direction in picture space makes the probability of the  cat  class increase, and then they give a little push (in other words, they perturb the input) in that direction. The new, modified image is mis-recognized as a cat.But what if there were no gradient   what if an infinitesimal modification to the image caused no change in the output of the model  This seems to provide some defense because the attacker does not know which way to  push  the image.We can easily imagine some very trivial ways to get rid of the gradient. For example, most image classification models can be run in two modes: one mode where they output just the identity of the most likely class, and one mode where they output probabilities. If the model s output is  99.9% airplane, 0.1% cat , then a little tiny change to the input gives a little tiny change to the output, and the gradient tells us which changes will increase the probability of the  cat  class. If we run the model in a mode where the output is just  airplane , then a little tiny change to the input will not change the output at all, and the gradient does not tell us anything.Let s run a thought experiment to see how well we could defend our model against adversarial examples by running it in  most likely class  mode instead of  probability mode.  The attacker no longer knows where to go to find inputs that will be classified as cats, so we might have some defense. Unfortunately, every image that was classified as a cat before is still classified as a cat now. If the attacker can guess which points are adversarial examples, those points will still be misclassified. We haven t made the model more robust; we have just given the attacker fewer clues to figure out where the holes in the models defense are.Even more unfortunately, it turns out that the attacker has a very good strategy for guessing where the holes in the defense are. The attacker can train their own model, a smooth model that has a gradient, make adversarial examples for their model, and then deploy those adversarial examples against our non-smooth model. Very often, our model will misclassify these examples too. In the end, our thought experiment reveals that hiding the gradient didn t get us anywhere.The defense strategies that perform gradient masking typically result in a model that is very smooth in specific directions and neighborhoods of training points, which makes it harder for the adversary to find gradients indicating good candidate directions to perturb the input in a damaging way for the model. However, the adversary can train a substitute model: a copy that imitates the defended model by observing the labels that the defended model assigns to inputs chosen carefully by the adversary.A procedure for performing such a model extraction attack was introduced in the black-box attacks paper. The adversary can then use the substitute model s gradients to find adversarial examples that are misclassified by the defended model as well. In the figure above, reproduced from the discussion of gradient masking found in Towards the Science of Security and Privacy in Machine Learning, we illustrate this attack strategy with a one-dimensional ML problem. The gradient masking phenomenon would be exacerbated for higher dimensionality problems, but harder to depict.We find that both adversarial training and defensive distillation accidentally perform a kind of gradient masking. Neither algorithm was explicitly designed to perform gradient masking, but gradient masking is apparently a defense that machine learning algorithms can invent relatively easily when they are trained to defend themselves and not given specific instructions about how to do so. If we transfer adversarial examples from one model to a second model that was trained with either adversarial training or defensive distillation, the attack often succeeds, even when a direct attack on the second model would fail. This suggests that both training techniques do more to flatten out the model and remove the gradient than to make sure it classifies more points correctly.Adversarial examples are hard to defend against because it is difficult to construct a theoretical model of the adversarial example crafting process. Adversarial examples are solutions to an optimization problem that is non-linear and non-convex for many ML models, including neural networks. Because we don t have good theoretical tools for describing the solutions to these complicated optimization problems, it is very hard to make any kind of theoretical argument that a defense will rule out a set of adversarial examples.Adversarial examples are also hard to defend against because they require machine learning models to produce good outputs for every possible input. Most of the time, machine learning models work very well but only work on a very small amount of all the many possible inputs they might encounter.Every strategy we have tested so far fails because it is not adaptive: it may block one kind of attack, but it leaves another vulnerability open to an attacker who knows about the defense being used. Designing a defense that can protect against a powerful, adaptive attacker is an important research area.Adversarial examples show that many modern machine learning algorithms can be broken in surprising ways. These failures of machine learning demonstrate that even simple algorithms can behave very differently from what their designers intend. We encourage machine learning researchers to get involved and design methods for preventing adversarial examples, in order to close this gap between what designers intend and how algorithms behave. If you\\'re interested in working on adversarial examples, consider joining OpenAI.To learn more about machine learning security, follow Ian and Nicolas\\'s machine learning security blog cleverhans.io.',\n",
       " \"The OpenAI team is now 45 people. Together, we're pushing the frontier of AI capabilities   whether by validating novel ideas, creating new software systems, or deploying machine learning on robots. We continue to look for creative, motivated researchers and engineers to help us achieve our goals.Welcome to everyone who's joined since our last team update!The following former OpenAI interns have joined us full-time: Catherine Olsson, Jonathan Ho, Paul Christiano, Peter Chen, Prafulla Dhariwal, Rein Houthooft, and Rocky Duan.\",\n",
       " \"Reinforcement learning algorithms can break in surprising, counterintuitive ways. In this post we'll explore one failure mode, which is where you misspecify your reward function.At OpenAI, we've recently started using Universe, our software for measuring and training AI agents, to conduct new RL experiments. Sometimes these experiments illustrate some of the issues with RL as currently practiced. In the following example we'll highlight what happens when a misspecified reward function encourages an RL agent to subvert its environment by prioritizing the acquisition of reward signals above other measures of success.Designing safe AI systems will require us to design algorithms that don't attempt to do this, and will teach us to specify and shape goals in such a way they can't be misinterpreted by our AI agents.One of the games we've been training on is CoastRunners. The goal of the game - as understood by most humans - is to finish the boat race quickly and (preferably) ahead of other players. CoastRunners does not directly reward the player's progression around the course, instead the player earns higher scores by hitting targets laid out along the route.We assumed the score the player earned would reflect the informal goal of finishing the race, so we included the game in an internal benchmark designed to measure the performance of reinforcement learning systems on racing games. However, it turned out that the targets were laid out in such a way that the reinforcement learning agent could gain a high score without having to finish the course. This led to some unexpected behavior when we trained an RL agent to play the game.The RL agent finds an isolated lagoon where it can turn in a large circle and repeatedly knock over three targets, timing its movement so as to always knock over the targets just as they repopulate. Despite repeatedly catching on fire, crashing into other boats, and going the wrong way on the track, our agent manages to achieve a higher score using this strategy than is possible by completing the course in the normal way. Our agent achieves a score on average 20 percent higher than that achieved by human players.While harmless and amusing in the context of a video game, this kind of behavior points to a more general issue with reinforcement learning: it is often difficult or infeasible to capture exactly what we want an agent to do, and as a result we frequently end up using imperfect but easily measured proxies. Often this works well, but sometimes it leads to undesired or even dangerous actions. More broadly it contravenes the basic engineering principle that systems should be reliable and predictable. We've also explored this issue at greater length in our research paper Concrete Problems on AI Safety.How can we avoid such problems  Aside from being careful about designing reward functions, several research directions OpenAI is exploring may help to reduce cases of misspecified rewards:These methods may have their own shortcomings. For example, transfer learning involves extrapolating a reward function for a new environment based on reward functions from many similar environments. This extrapolation could itself be faulty   for example, an agent trained on many racing video games where driving off the road has a small penalty, might incorrectly conclude that driving off the road in a new, higher stakes setting is not a big deal. More subtly, if the reward extrapolation process involves neural networks, adversarial examples in that network could lead a reward function that has  unnatural  regions of high reward that do not correspond to any reasonable real-world goal.Solving these issues will be complex. Our hope is that Universe will enable us to both discover and address new failure modes at a rapid pace, and eventually to develop systems whose behavior we can be truly confident in.Get in touch with the authors of this post: Dario, Jack\",\n",
       " \"We're releasing Universe, a software platform for measuring and training an AI's general intelligence across the world's supply of games, websites and other applications.Universe allows an AI agent to use a computer like a human does: by looking at screen pixels and operating a virtual keyboard and mouse. We must train AI systems on the full range of tasks we expect them to solve, and Universe lets us train a single agent on any task a human can complete with a computer.In April, we launched Gym, a toolkit for developing and comparing reinforcement learning (RL) algorithms. With Universe, any program can be turned into a Gym environment. Universe works by automatically launching the program behind a VNC remote desktop   it doesn't need special access to program internals, source code, or bot APIs.Today's release consists of a thousand environments including Flash games, browser tasks, and games like slither.io and GTA V. Hundreds of these are ready for reinforcement learning, and almost all can be freely run with the universe Python library as follows:The sample code above will start your AI playing the Dusk Drive Flash game. Your AI will be given frames like the above 60 times per second. You'll need to have Docker and universe installed.Our goal is to develop a single AI agent that can flexibly apply its past experience on Universe environments to quickly master unfamiliar, difficult environments, which would be a major step towards general intelligence. There are many ways to help: giving us permission on your games, training agents across Universe tasks, (soon) integrating new games, or (soon) playing the games.With support from EA, Microsoft Studios, Valve, Wolfram, and many others, we've already secured permission for Universe AI agents to freely access games and applications such as Portal, Fable Anniversary, World of Goo, RimWorld, Slime Rancher, Shovel Knight, SpaceChem, Wing Commander III, Command & Conquer: Red Alert 2, Syndicate, Magic Carpet, Mirror's Edge, Sid Meier's Alpha Centauri, and Wolfram Mathematica. We look forward to integrating these and many more.The area of artificial intelligence has seen rapid progress over the last few years. Computers can now see, hear, and translate languages with unprecedented accuracies. They are also learning to generate images, sound, and text. A reinforcement learning system, AlphaGo, defeated the world champion at Go. However, despite all of these advances, the systems we're building still fall into the category of  Narrow AI    they can achieve super-human performance in a specific domain, but lack the ability to do anything sensible outside of it. For instance, AlphaGo can easily defeat you at Go, but you can't explain the rules of a different board game to it and expect it to play with you.Systems with general problem solving ability   something akin to human common sense, allowing an agent to rapidly solve a new hard task   remain out of reach. One apparent challenge is that our agents don't carry their experience along with them to new tasks. In a standard training regime, we initialize agents from scratch and let them twitch randomly through tens of millions of trials as they learn to repeat actions that happen to lead to rewarding outcomes. If we are to make progress towards generally intelligent agents, we must allow them to experience a wide repertoire of tasks so they can develop world knowledge and problem solving strategies that can be efficiently reused in a new task.The Atari 2600 game  Montezuma's Revenge,  which is notoriously difficult to learn with reinforcement learning. A human player can immediately see that they control the person, that the skull is probably bad to touch, or that it is probably a good idea to collect the key. An AI agent starting from scratch and without any transfer from past experience is forced to discover the solution through a trial and error approach that may require millions of attempts.Universe exposes a wide range of environments through a common interface: the agent operates a remote desktop by observing pixels of a screen and producing keyboard and mouse commands. The environment exposes a VNC server and the universe library turns the agent into a VNC client.Our design goal for universe was to support a single Python process driving 20 environments in parallel at 60 frames per second. Each screen buffer is 1024x768, so naively reading each frame from an external process would take 3GB/s of memory bandwidth. We wrote a batch-oriented VNC client in Go, which is loaded as a shared library in Python and incrementally updates a pair of buffers for each environment. After experimenting with many combinations of VNC servers, encodings, and undocumented protocol options, we now routinely drive dozens of environments at 60 frames per second with 100ms latency   almost all due to server-side encoding.Here are some important properties of our current implementation:General. An agent can use this interface (which was originally designed for humans) to interact with any existing computer program without requiring an emulator or access to the program's internals. For instance, it can play any computer game, interact with a terminal, browse the web, design buildings in CAD software, operate a photo editing program, or edit a spreadsheet.Familiar to humans. Since people are already well versed with the interface of pixels/keyboard/mouse, humans can easily operate any of our environments. We can use human performance as a meaningful baseline, and record human demonstrations by simply saving VNC traffic. We've found demonstrations to be extremely useful in initializing agents with sensible policies with behavioral cloning (i.e. use supervised learning to mimic what the human does), before switching to RL to optimize for the given reward function.VNC as a standard. Many implementations of VNC are available online and some are packaged by default into the most common operating systems, including OSX. There are even VNC implementations in JavaScript, which allow humans to provide demonstrations without installing any new software   important for services like Amazon Mechanical Turk.Easy to debug. We can observe our agent while it is training or being evaluated   we just attach a VNC client to the environment's (shared) VNC desktop. We can also save the VNC traffic for future analysis.We were all quite surprised that we could make VNC work so well. As we scale to larger games, there's a decent chance we'll start using additional backend technologies. But preliminary signs indicate we can push the existing implementation far: with the right settings, our client can coax GTA V to run at 20 frames per second over the public internet.We have already integrated a large number of environments into Universe, and view these as just the start. Each environment is packaged as a Docker image and hosts two servers that communicate with the outside world: the VNC server which sends pixels and receives keyboard/mouse commands, and a WebSocket server which sends the reward signal for reinforcement learning tasks (as well as any auxiliary information such as text, or diagnostics) and accepts control messages (such as the specific environment ID to run).Universe includes the Atari 2600 games from the Arcade Learning Environment. These environments now run asynchronously inside the quay.io/openai/universe.gym-core Docker image and allow the agent to connect over the network, which means the agent must handle lag and low frame rates. Running over a local network in the cloud, we usually see 60 frames per second, observation lags of 20ms, and action lags of 10ms; over the public internet this drops to 20 frames per second, 80ms observation lags, and 30ms action lags.We turned to Flash games as a starting point for scaling Universe   they are pervasive on the Internet, generally feature richer graphics than Atari, but are still individually simple. We've sifted through over 30,000 so far, and estimate there's an order of magnitude more.Our initial Universe release includes 1,000 Flash games (100 with reward functions), which we distribute in the quay.io/openai/universe.flashgames Docker image with consent from the rightsholders. This image starts a TigerVNC server and boots a Python control server, which uses Selenium to open a Chrome browser to an in-container page with the desired game, and automatically clicks through any menus needed to start the game.Extracting rewards. While environments without reward functions can be used for unsupervised learning or to generate human demonstrations, RL needs a reward function. Unlike with the Atari games, we can't simply read out success criteria from the process memory, as there is too much variation in how each game stores this information. Fortunately, many games have an on-screen score which we can use as a reward function, as long as we can parse it. While off-the-shelf OCR such as Tesseract performs great on standard fonts with clean backgrounds, it struggles with the diverse fonts, moving backgrounds, flashy animations, or occluding objects common in many games. We developed a convolutional neural network-based OCR model that runs inside the Docker container's Python controller, parses the score (from a screen buffer maintained via a VNC self-loop), and communicates it over the WebSocket channel to the agent.Humanity has collectively built the Internet into an immense treasure trove of information, designed for visual consumption by humans. Universe includes browser-based environments which require AI agents to read, navigate, and use the web just like people   using pixels, keyboard, and mouse.Today, our agents are mostly learning to interact with common user interface elements like buttons, lists and sliders, but in the future they could complete complex tasks, such as looking up things they don't know on the internet, managing your email or calendar, completing Khan Academy lessons, or working on Amazon Mechanical Turk and CrowdFlower tasks.Mini World of Bits. We first set out to create a new benchmark that captures the salient challenges of browser interactions in a simple setting. We call this benchmark Mini World of Bits. We think of it as an analogue to MNIST, and believe that mastering these environments provides valuable signal towards models and training techniques that will perform well on full websites and more complex tasks. Our initial Mini World of Bits benchmark consists of 80 environments that range from simple (e.g. click a specific button) to difficult (e.g. reply to a contact in a simulated email client).Real-world browser tasks. We've begun work on more realistic browser tasks. The agent takes an instruction, and performs a sequence of actions on a website. One such environment hands the agent details of a desired flight booking, and then requires it to manipulate a user interface to search for the flight. (We use cached recordings of these sites to avoid spamming them, or booking lots of real flights.)This infrastructure is general-purpose: we can integrate any game, website, or application which can run in a Docker container (most convenient) or a Windows virtual machine (less convenient). We'd like the community's help to continue extending the breadth of Universe environments, including completing the integrations of our partners' games, Android apps (emulators can run inside of Docker), fold.it, Unity games, HTML5 games, online educational games, and really anything else people think of.Microsoft's Project Malmo team will be integrating with Universe, and we look forward to supporting other AI frameworks as well.Despite the huge variety, running Universe environments requires minimal setup. You'll need only to install Docker and universe:We package each collection of similar environments into a  runtime , which is a server exposing two ports: 5900 (used for the VNC protocol to exchange pixels/keyboard/mouse) and 15900 (used for a WebSocket control protocol). For example, the quay.io/openai/universe.flashgames Docker image is a runtime that can serve many different Flash game environments.Starting a runtime. You can boot your first runtime from the console as follows:This will download and run the Flash games Docker container. You can view and control the remote desktop by connecting your own VNC viewer to port 5900, such as via TurboVNC or the browser-based VNC client served via the webserver on port 15900. The default password is openai. OSX also has a native VNC viewer which can be accessed by running open vnc://localhost:5900 in Terminal. (Unfortunately, the OSX viewer doesn't implement Tight encoding, which is the best option for bigger games.)Writing your own agent. You can write your own agent quite easily, using your favorite framework such as TensorFlow or Theano. (We've provided a starter TensorFlow agent.) At each time step, the agent's observation includes a NumPy pixel array, and the agent must emit a list of VNC events (mouse/keyboard actions). For example, the following agent will activate Dusk Drive and press forward constantly:You can keep your own VNC connection open, and watch the agent play, or even use the keyboard and mouse alongside the agent in a human/agent co-op mode.Environment management. Because environments run as server processes, they can run on remote machines, possibly within a cluster or even over the public internet. We've documented a few ways to manage remote runtimes. At OpenAI, we use an  allocator  HTTP service, which provisions runtimes across a Kubernetes cluster on demand, and which we can use to connect a single agent process to hundreds of simultaneous environments.Universe agents must deal with real-world griminess that traditional RL agents are shielded from: agents must run in real-time and account for fluctuating action and observation lag. While the full complexity of Universe is designed to be out of reach of current techniques, we also have ensured it's possible to make progress today.Universe Pong. Our first goal was solving gym-core.PongDeterministic-v3. Pong is one of the easiest Atari games, but it had the potential to be intractable as a Universe task, since the agent has to learn to perform very precise maneuvers at 4x realtime (as the environment uses a standard frameskip of 4). We used this environment to validate that Universe's variable latencies still allowed for learning precise and rapid reactions. Today's release includes universe-starter-agent, which takes an hour to train to a Pong score of +17 out of 21. Humans playing the same version of Pong were only able reach a score of -11 on a scale between -21 and 21, due to the game's high speed.Trained agent playing gym-core.PongDeterministic-v3. The video plays at real time.Additional experiments. We applied RL to several racing Flash games, which worked after applying some standard tricks such as reward normalization. Some browser tasks where we tried RL had difficult exploration problems, but were solvable with behavioral cloning from human demonstration data.Some of our successful agents are shown below. While solving Universe will require an agent far outside the reach of current techniques, these videos show that many interesting Universe environments can be fruitfully approached with today's algorithms.Remote training. We've also experimented with slither.io agents on our physical infrastructure (which has access to Titan X GPUs) and environments in the cloud. Generally, the agent will control 32 simultaneous environments at 5 frames per second   observations are available much more frequently, but lower framerates help today's RL algorithms, which struggle with dependencies over many timesteps.\\nOur typical setup for training an agent over the public internet, with 32 environments running in the cloud provisioned by our allocator instance.Our agent's  reaction time  averages around 150ms over the public internet: 110ms for an observation to arrive, 10ms to compute the action, and 30ms for the action to take effect. (For comparison, human reaction time averages around 250ms.) Reaction times drop to 80ms over a local network, and 40ms within a single machine.Research progress requires meaningful performance measurement. In upcoming weeks, we'll release a transfer learning benchmark, allowing researchers to determine if they are making progress on general problem solving ability.Universe draws inspiration from the history of the ImageNet dataset in the Computer Vision community. Fei-Fei Li and her collaborators deliberately designed the ImageNet benchmark to be nearly impossible, but error rates have dropped rapidly from 28% in 2010 to 3% in 2016, which reaches (or in some cases even surpasses) human-level performance.If the AI community does the same with Universe, then we will have made real progress towards systems with broad, general intelligence.Universe will only succeed with the community's help. There are many ways to contribute (and one particularly great way is to join us):If your program would yield good training tasks for an AI then we'd love your permission to package it in Universe. Good candidates have an on-screen number (such as a game score) which can be parsed as a reward, or well-defined objectives, either natively or definable by the user.AI advances require the entire community to collaborate, and we welcome the community's help in training agents across these tasks. We've released a starter agent which should be a helpful starting point for building your own agents. In upcoming weeks, we'll release the sub-benchmarks we think are the right places to start.We have many more environments waiting to be integrated than we can handle on our own. In upcoming weeks, we'll release our environment integration tools, so anyone can contribute new environment integrations. In the meanwhile, we'll be running a beta for environment integrators.We're compiling a large dataset of human demonstrations on Universe environments, which will be released publicly. If you'd like to play games for the good of science, please sign up for our beta.The following partners have been key to creating Universe: EA, Valve, Microsoft, NVIDIA, Kongregate, Newgrounds, Yacht Club Games, Zachtronics, Ludeon Studios, Monomi Park, 2D Boy, Adam Reagle, Alvin Team, Rockspro, Anubhav Sharma, Arkadium, Beast Games, Char Studio, Droqen, Percy Pea, deeperbeige, Denny Menato, Dig Your Own Grave, Free World Group, Gamesheep, Hamumu Software, Hemisphere Games, Icy Lime, Insane Hero, inRegular Games, JackSmack, Nocanwin, Joe Willmott, Johnny Two Shoes, The Gamest Studio, L szl  Czigl dszky, Madalin Games, Martian Games, Mateusz Skutnik, Mikalay Radchuk, Neutronized, Nitrome, ooPixel, PacoGames, Pixelante, Plemsoft, Rob Donkin, robotJam, Rumble Sushi 3D, SFB Games, Simian Logic, Smiley Gamer, Sosker, tequibo, kometbomb, ThePodge, Vasco Freitas, Vitality Games, Wolve Games, Xform Games, XGen Studios\",\n",
       " \"We re working with Microsoft to start running most of our large-scale experiments on Azure. This will make Azure the primary cloud platform that OpenAI is using for deep learning and AI, and will let us conduct more research and share the results with the world.One of the most important factors for accelerating our progress is accessing more and faster computers; this is particularly true for emerging AI technologies like reinforcement learning and generative models. Azure has impressed us by building hardware configurations optimized for deep learning   they offer K80 GPUs with InfiniBand interconnects at scale. We re also excited by their roadmap, which should soon bring Pascal GPUs onto their cloud.In the coming months we will use thousands to tens of thousands of these machines to increase both the number of experiments we run and the size of the models we train.We ll share the results of this partnership with everyone: along with publishing our research results, we'll continue releasing open-source software making it easier for people to run large-scale AI workloads on the cloud. We'll also be giving feedback to the Microsoft team so that Azure's capabilities keep pace with our understanding of AI.It s great to work with another organization that believes in the importance of democratizing access to AI. We re looking forward to accelerating the AI community through this partnership.\",\n",
       " \"Our first group learning experiment! Last week we hosted over a hundred and fifty AI practitioners in our offices for our first self-organizing conference on machine learning. The goal was to accelerate AI research by bringing a diverse group of people together and making it easy for them to educate each other and generate new ideas. To achieve this we sought to build an entire event around the chance hallway conversations, serendipitous lunches and inspiring encounters that people have at traditional conferences.The format worked. PHD students talked to professors, hobbyists talked to full-time researchers, and designers mingled with neuroscientists; most importantly, many people left the event with new research ideas. Participants identified issues ranging from the need for a greater theoretical underpinning within robotics, to how we might make use of neuroscience to accelerate AI development, to ways to increase the diversity of the AI community. Minutes from some of these meetings are available.The backbone of typical conferences consists of keynotes, plenaries, and panels, turning many of the attendees into passive spectators. At SOCML, the backbone of the event was about the interaction between participants, and they were able to form sessions, choose moderators, give impromptu lectures, and debug each other's problems, without much administration. Everyone taught everyone else and everyone learned from everyone else.The self-organizing nature makes this sort of conference reasonably easy and affordable to host, and brings talented minds together to work on urgent issues. If you're keen to host a self-organizing conference of your own we ll be gathering feedback and adding some tips and tricks to the wiki in the coming weeks. Good luck!\",\n",
       " \"Deep learning is an empirical science, and the quality of a group's infrastructure is a multiplier on progress. Fortunately, today's open-source ecosystem makes it possible for anyone to build great deep learning infrastructure.In this post, we'll share how deep learning research usually proceeds, describe the infrastructure choices we've made to support it, and open-source kubernetes-ec2-autoscaler, a batch-optimized scaling manager for Kubernetes. We hope you find this post useful in building your own deep learning infrastructure.A typical deep learning advance starts out as an idea, which you test on a small problem. At this stage, you want to run many ad-hoc experiments quickly. Ideally, you can just SSH into a machine, run a script in screen, and get a result in less than an hour.Making the model really work usually requires seeing it fail in every conceivable way and finding ways to fix those limitations. (This is similar to building any new software system, where you'll run your code many times to build an intuition for how it behaves.)You need to inspect your models from many angles to gain intuition for what they're actually learning. This reinforcement learning agent (controlling the right paddle) from Dario Amodei achieves a high Pong score, but when you watch it play you'll notice it just sits in one place.So deep learning infrastructure must allow users to flexibly introspect models, and it's not enough to just expose summary statistics.Once the model shows sufficient promise, you'll scale it up to larger datasets and more GPUs. This requires long jobs that consume many cycles and last for multiple days. You'll need careful experiment management, and to be extremely thoughtful about your chosen range of hyperparameters.The early research process is unstructured and rapid; the latter is methodical and somewhat painful, but it's all absolutely necessary to get a great result.The paper Improved Techniques for Training GANs began with Tim Salimans devising several ideas for improving Generative Adversarial Network training. We'll describe the simplest of these ideas (which happened to produce the best-looking samples, though not the best semi-supervised learning).GANs consist of a generator and a discriminator network. The generator tries to fool the discriminator, and the discriminator tries to distinguish between generated data and real data. Intuitively, a generator which can fool every discriminator is quite good. But there is a hard-to-fix failure mode: the generator can  collapse  by always outputting exactly the same (likely realistic-looking!) sample.Tim had the idea to give discriminator an entire minibatch of samples as input, rather than just one sample. Thus the discriminator can tell whether the generator just constantly produces a single image. With the collapse discovered, gradients will be sent to the generator to correct the problem.The next step was to prototype the idea on MNIST and CIFAR-10. This required prototyping a small model as quickly as possible, running it on real data, and inspecting the result. After some rapid iteration, Tim got very encouraging CIFAR-10 samples   pretty much the best samples we'd seen on this dataset.However, deep learning (and AI algorithms in general) must be scaled to be truly impressive   a small neural network is a proof of concept, but a big neural network actually solves the problem and is useful. So Ian Goodfellow dug into scaling the model up to work on ImageNet.Our model learning to generate ImageNet imagesWith a larger model and dataset, Ian needed to parallelize the model across multiple GPUs. Each job would push multiple machines to 90% CPU and GPU utilization, but even then the model took many days to train. In this regime, every experiment became precious, and he would meticulously log the results of each experiment.Ultimately, while the results were good, they were not as good as we hoped. We've tested many hypotheses as to why, but still haven't cracked it. Such is the nature of science.A sample of our TensorFlow codeThe vast majority of our research code is written in Python, as reflected in our open-source projects. We mostly use TensorFlow (or Theano in special cases) for GPU computing; for CPU we use those or Numpy. Researchers also sometimes use higher-level frameworks like Keras on top of TensorFlow.Like much of the deep learning community, we use Python 2.7. We generally use Anaconda, which has convenient packaging for otherwise difficult packages such as OpenCV and performance optimizations for some scientific libraries.For an ideal batch job, doubling the number of nodes in your cluster will halve the job's runtime. Unfortunately, in deep learning, people usually see very sublinear speedups from many GPUs. Top performance thus requires top-of-the-line GPUs. We also use quite a lot of CPU for simulators, reinforcement learning environments, or small-scale models (which run no faster on a GPU).nvidia-smi showing fully-loaded Titan XsAWS generously agreed to donate a large amount of compute to us. We're using them for CPU instances and for horizontally scaling up GPU jobs. We also run our own physical servers, primarily running Titan X GPUs. We expect to have a hybrid cloud for the long haul: it's valuable to experiment with different GPUs, interconnects, and other techniques which may become important for the future of deep learning.htop on the same physical box showing plenty of spare CPU. We generally run our CPU-intensive workloads separately from our GPU-intensive ones.We approach infrastructure like many companies treat product: it must present a simple interface, and usability is as important as functionality. We use a consistent set of tools to manage all of our servers and configure them as identically as possible.Snippet of our Terraform config for managing Auto Scaling groups. Terraform creates, modifies, or destroys your running cloud resources to match your configuration files.We use Terraform to set up our AWS cloud resources (instances, network routes, DNS records, etc). Our cloud and physical nodes run Ubuntu and are configured with Chef. For faster spinup times, we pre-bake our cluster AMIs using Packer. All our clusters use non-overlapping IP ranges and are interconnected over the public internet with OpenVPN on user laptops, and strongSwan on physical nodes (which act as AWS Customer Gateways).We store people's home directories, data sets, and results on NFS (on physical hardware) and EFS/S3 (on AWS).Scalable infrastructure often ends up making the simple cases harder. We put equal effort into our infrastructure for small- and large-scale jobs, and we're actively solidifying our toolkit for making distributed use-cases as accessible as local ones.We provide a cluster of SSH nodes (both with and without GPUs) for ad-hoc experimentation, and run Kubernetes as our cluster scheduler for physical and AWS nodes. Our cluster spans 3 AWS regions   our jobs are bursty enough that we'll sometimes hit capacity on individual regions.Kubernetes requires each job to be a Docker container, which gives us dependency isolation and code snapshotting. However, building a new Docker container can add precious extra seconds to a researcher's iteration cycle, so we also provide tooling to transparently ship code from a researcher's laptop into a standard image.Model learning curves in TensorBoardWe expose Kubernetes's flannel network directly to researchers' laptops, allowing users seamless network access to their running jobs. This is especially useful for accessing monitoring services such as TensorBoard. (Our initial approach   which is cleaner from a strict isolation perspective   required people to create a Kubernetes Service for each port they wanted to expose, but we found that it added too much friction.)Our workload is bursty and unpredictable: a line of research can go quickly from single-machine experimentation to needing 1,000 cores. For example, over a few weeks, one experiment went from an interactive phase on a single Titan X, to an experimental phase on 60 Titan Xs, to needing nearly 1600 AWS GPUs. Our cloud infrastructure thus needs to dynamically provision Kubernetes nodes.It's easy to run Kubernetes nodes in Auto Scaling groups, but it's harder to correctly manage the size of those groups. After a batch job is submitted, the cluster knows exactly what resources it needs, and should allocate those directly. (In contrast, AWS's Scaling Policies will spin up new nodes piecemeal until resources are no longer exhausted, which can take multiple iterations.) Also, the cluster needs to drain nodes before terminating them to avoid losing in-flight jobs.It's tempting to just use raw EC2 for big batch jobs, and indeed that's where we started. However, the Kubernetes ecosystem adds quite a lot of value: low-friction tooling, logging, monitoring, ability to manage physical nodes separately from the running instances, and the like. Making Kubernetes autoscale correctly was easier than rebuilding this ecosystem on raw EC2.We're releasing kubernetes-ec2-autoscaler, a batch-optimized scaling manager for Kubernetes. It runs as a normal Pod on Kubernetes and requires only that your worker nodes are in Auto Scaling groups.The Launch Configurations for our Kubernetes clusterThe autoscaler works by polling the Kubernetes master's state, which contains everything needed to calculate the cluster resource ask and capacity. If there's excess capacity, it drains the relevant nodes and ultimately terminates them. If more resources are needed, it calculates what servers should be created and increases your Auto Scaling group sizes appropriately (or simply uncordons drained nodes, which avoids new node spinup time).kubernetes-ec2-autoscaler handles multiple Auto Scaling groups, resources beyond CPU (memory and GPUs), and fine-grained constraints on your jobs such as AWS region and instance size. Additionally, bursty workloads can lead to Auto Scaling Groups timeouts and errors, since (surprisingly!) even AWS does not have infinite capacity. In these cases, kubernetes-ec2-autoscaler detects the error and overflows to a secondary AWS region.Our infrastructure aims to maximize the productivity of deep learning researchers, allowing them to focus on the science. We're building tools to further improve our infrastructure and workflow, and will share these in upcoming weeks and months. We welcome help to make this go even faster!\",\n",
       " \"The latest information about the Unconference is now available at the Unconference wiki, which will be periodically updated with more information for attendees.Machine learning is moving incredibly quickly. To keep up, many practitioners spend several weeks a year at conferences. However, conference presentations are all on work submitted months prior, meaning that people are already intimately familiar with the content (and it's often already been surpassed).We'd like to try instead hosting an event focused on the most valuable part of any conference: the people. Please join us for our first Machine Learning Unconference, an experimental gathering driven by its participants rather than an organizing committee.The unconference will be a free event at the OpenAI office in San Francisco on Friday and Saturday, October 7-8, 2016. We welcome participants from around the globe. As of August 22, we have finished accepting applications for the unconference.Conferences play two main roles: social gathering and publication. We feel that these two roles are somewhat orthogonal and can be better served separately.At most conferences, the unstructured social time, such as mealtimes, is what people find valuable. We want to maximize the amount of interactivity in a social gathering of researchers. A minimally-structured unconference provides an opportunity for an experiment, to see what kinds of new formats attendees will develop, and find out which of these are most effective.We also have ideas for how to improve the paper reviewing and publication system, which we are developing separately.It would go against the spirit of the unconference for us to provide a specific schedule, but we suggest attendees use the Gitter chat to plan a few topics of discussion ahead of time.All attendees are also encouraged to bring a poster describing machine learning work that would be interesting or useful for other attendees to learn about. (We'll follow up with more details on format via email.)Anyone who is involved in machine learning research or applications is welcome. We especially expect to see:We're particularly excited to support women, minorities, and members of other groups underrepresented in machine learning. OpenAI will provide a limited number of travel and lodging grants for members of these groups.This event is primarily intended for people with technical fluency in machine learning to help each other advance the state of the art. If you are interested in learning about the basics of the field, there are plenty of other great events we'd suggest.OpenAI is sponsoring and hosting the event, but the event itself has no official organization   it's just people who find this invitation interesting, meeting to discuss machine learning.OpenAI will provide lunch and snacks. Participants are responsible for arranging their own travel and lodging.By attending, you agree to abide by our code of conduct.We'd like to welcome everyone, but we only have capacity for 150 people. We'll tune the acceptance list to ensure an interesting conference with a diversity of people and perspectives.Here's a somewhat representative sample of people you'll get to meet at the unconference, based on early signups:You can discuss plans and logistics with other attendees in the Gitter chat, or email Ian with any questions.\",\n",
       " \"We've hired more great people to help us achieve our goals. Welcome, everyone!We're also pumped to be working with the following people for a more limited period of time:\",\n",
       " '\\nImpactful scientific work requires working on the right problems\\n  problems which are not just interesting, but whose solutions\\n matter. In this post, we list several problem areas likely to be\\nimportant both for advancing AI and for its long-run impact on\\n society.  We see these problems as having either very broad implications, or\\naddressing important emerging consequences of AI development. If you\\nare a strong machine learning expert and wish to start an effort on\\none of these problems at OpenAI,\\nplease submit an application. ',\n",
       " \"We (along with researchers from Berkeley and Stanford) are co-authors on today's paper led by Google Brain researchers, Concrete Problems in AI Safety. The paper explores many research problems around ensuring that modern machine learning systems operate as intended. (The problems are very practical, and we've already seen some being integrated into OpenAI Gym.)Advancing AI requires making AI systems smarter, but it also requires preventing accidents   that is, ensuring that AI systems do what people actually want them to do. There's been an increasing focus on safety research from the machine learning community, such as a recent paper from DeepMind and FHI. Still, many machine learning researchers have wondered just how much safety research can be done today.The authors discuss five areas:Many of the problems are not new, but the paper explores them in the context of cutting-edge systems. We hope they'll inspire more people to work on AI safety research, whether at OpenAI or elsewhere.We're particularly excited to have participated in this paper as a cross-institutional collaboration. We think that broad AI safety collaborations will enable everyone to build better machine learning systems. Let us know if you have a future paper you'd like to collaborate on!\",\n",
       " \"OpenAI s mission is to build safe AI, and ensure AI's benefits are as widely and evenly distributed as possible. We re trying to build AI as part of a larger community, and we want to share our plans and capabilities along the way. We re also working to solidify our organization's governance structure and will share our thoughts on that later this year.Defining a metric for intelligence is tricky, but we need one to measure our progress and focus our research. We're thus building a living metric which measures how well an agent can achieve its user s intended goal in a wide range of environments.The metric will consist of a variety of OpenAI Gym environments with a unified action and observation space (so a single agent can run across all of them), including games, robotics, and language-based tasks. Our implementation will evolve over time, and we ll keep the community updated along the way.A significant fraction of our research bandwidth is being spent on fundamental research. We ll always be developing and testing new ideas, especially those that don t fit neatly into our current worldview. This is important   our current ideas will not be enough to achieve our long-term goal.We ve also formed teams around specific projects. The intention isn t just to solve these problems, but to develop general learning algorithms in the process. These algorithms will, in turn, help us build agents that are more capable according to our metric. These projects are:We're working to enable a physical robot (off-the-shelf; not manufactured by OpenAI) to perform basic housework. There are existing techniques for specific tasks, but we believe that learning algorithms can eventually be made reliable enough to create a general-purpose robot. More generally, robotics is a good testbed for many challenges in AI.We plan to build an agent that can perform a complex task specified by language, and ask for clarification about the task if it s ambiguous. Today, there are promising algorithms for supervised language tasks such as question answering, syntactic parsing and machine translation but there aren t any for more advanced linguistic goals, such as the ability to carry a conversation, the ability to fully understand a document, and the ability to follow complex instructions in natural language. We expect to develop new learning algorithms and paradigms to tackle these problems.We aim to train an agent capable enough to solve any game in our initial metric. Games are virtual mini-worlds that are very diverse, and learning to play games quickly and well will require significant advances in generative models and reinforcement learning. (We are inspired by the pioneering work of DeepMind, who have produced impressive results in this area in the past few years.)Our projects and fundamental research all have shared cores, so progress on any is likely to benefit the others. Each captures a different aspect of goal-solving, and was chosen for its potential to significantly move our metric.We re just getting started on these projects, and the details may change as we gain additional data. We also expect to add new projects over time.If you re excited about any of the above, we'd love to hear from you, whether by discussing with others in the OpenAI community or joining us full-time.\",\n",
       " 'This post describes four projects that share a common theme of enhancing or using generative models, a branch of unsupervised learning techniques in machine learning.In addition to describing our work, this post will tell you a bit more about generative models: what they are, why they are important, and where they might be going.One of our core aspirations at OpenAI is to develop algorithms and techniques that endow computers with an understanding of our world.It\\'s easy to forget just how much you know about the world: you understand that it is made up of 3D environments, objects that move, collide, interact; people who walk, talk, and think; animals who graze, fly, run, or bark; monitors that display information encoded in language about the weather, who won a basketball game, or what happened in 1970.This tremendous amount of information is out there and to a large extent easily accessible   either in the physical world of atoms or the digital world of bits. The only tricky part is to develop models and algorithms that can analyze and understand this treasure trove of data.Generative models are one of the most promising approaches towards this goal. To train a generative model we first collect a large amount of data in some domain (e.g., think millions of images, sentences, or sounds, etc.) and then train a model to generate data like it. The intuition behind this approach follows a famous quote from Richard Feynman: What I cannot create, I do not understand. The trick is that the neural networks we use as generative models have a number of parameters significantly smaller than the amount of data we train them on, so the models are forced to discover and efficiently internalize the essence of the data in order to generate it.Generative models have many short-term applications. But in the long run, they hold the potential to automatically learn the natural features of a dataset, whether categories or dimensions or something else entirely.Let s make this more concrete with an example. Suppose we have some large collection of images, such as the 1.2 million images in the ImageNet dataset (but keep in mind that this could eventually be a large collection of images or videos from the internet or robots). If we resize each image to have width and height of 256 (as is commonly done), our dataset is one large 1,200,000x256x256x3 (about 200GB) block of pixels. Here are a few example images from this dataset:These images are examples of what our visual world looks like and we refer to these as \"samples from the true data distribution\". We now construct our generative model which we would like to train to generate images like this from scratch. Concretely, a generative model in this case could be one large neural network that outputs images and we refer to these as \"samples from the model\".One such recent model is the DCGAN network from Radford et al. (shown below). This network takes as input 100 random numbers drawn from a uniform distribution (we refer to these as a code, or latent variables, in red) and outputs an image (in this case 64x64x3 images on the right, in green). As the code is changed incrementally, the generated images do too   this shows the model has learned features to describe how the world looks, rather than just memorizing some examples.The network (in yellow) is made up of standard convolutional neural network components, such as deconvolutional layers (reverse of convolutional layers), fully connected layers, etc.:DCGAN is initialized with random weights, so a random code plugged into the network would generate a completely random image. However, as you might imagine, the network has millions of parameters that we can tweak, and the goal is to find a setting of these parameters that makes samples generated from random codes look like the training data. Or to put it another way, we want the model distribution to match the true data distribution in the space of images.Suppose that we used a newly-initialized network to generate 200 images, each time starting with a different random code. The question is: how should we adjust the network\\'s parameters to encourage it to produce slightly more believable samples in the future  Notice that we\\'re not in a simple supervised setting and don\\'t have any explicit desired targets for our 200 generated images; we merely want them to look real. One clever approach around this problem is to follow the Generative Adversarial Network (GAN) approach. Here we introduce a second discriminator network (usually a standard convolutional neural network) that tries to classify if an input image is real or generated. For instance, we could feed the 200 generated images and 200 real images into the discriminator and train it as a standard classifier to distinguish between the two sources. But in addition to that   and here\\'s the trick   we can also backpropagate through both the discriminator and the generator to find how we should change the generator\\'s parameters to make its 200 samples slightly more confusing for the discriminator. These two networks are therefore locked in a battle: the discriminator is trying to distinguish real images from fake images and the generator is trying to create images that make the discriminator think they are real. In the end, the generator network is outputting images that are indistinguishable from real images for the discriminator.There are a few other approaches to matching these distributions which we will discuss briefly below. But before we get there below are two animations that show samples from a generative model to give you a visual sense for the training process.\\nIn both cases the samples from the generator start out noisy and chaotic, and over time converge to have more plausible image statistics:This is exciting   these neural networks are learning what the visual world looks like! These models usually have only about 100 million parameters, so a network trained on ImageNet has to (lossily) compress 200GB of pixel data into 100MB of weights. This incentivizes it to discover the most salient features of the data: for example, it will likely learn that pixels nearby are likely to have the same color, or that the world is made up of horizontal or vertical edges, or blobs of different colors. Eventually, the model may discover many more complex regularities: that there are certain types of backgrounds, objects, textures, that they occur in certain likely arrangements, or that they transform in certain ways over time in videos, etc.Mathematically, we think about a dataset of examples x_1, \\\\ldots, x_n as samples from a true data distribution p(x). In the example image below, the blue region shows the part of the image space that, with a high probability (over some threshold) contains real images, and black dots indicate our data points (each is one image in our dataset). Now, our model also describes a distribution \\\\hat{p}_{\\\\theta}(x) (green) that is defined implicitly by taking points from a unit Gaussian distribution (red) and mapping them through a (deterministic) neural network   our generative model (yellow). Our network is a function with parameters \\\\theta, and tweaking these parameters will tweak the generated distribution of images. Our goal then is to find parameters \\\\theta that produce a distribution that closely matches the true data distribution (for example, by having a small KL divergence loss). Therefore, you can imagine the green distribution starting out random and then the training process iteratively changing the parameters \\\\theta to stretch and squeeze it to better match the blue distribution.\\nMost generative models have this basic setup, but differ in the details. Here are three popular examples of generative model approaches to give you a sense of the variation:All of these approaches have their pros and cons. For example, Variational Autoencoders allow us to perform both learning and efficient Bayesian inference in sophisticated probabilistic graphical models with latent variables (e.g. see DRAW, or Attend Infer Repeat for hints of recent relatively complex models). However, their generated samples tend to be slightly blurry. GANs currently generate the sharpest images but they are more difficult to optimize due to unstable training dynamics. PixelRNNs have a very simple and stable training process (softmax loss) and currently give the best log likelihoods (that is, plausibility of the generated data). However, they are relatively inefficient during sampling and don\\'t easily provide simple low-dimensional codes for images. All of these models are active areas of research and we are eager to see how they develop in the future!We\\'re quite excited about generative models at OpenAI, and have just released four projects that advance the state of the art. For each of these contributions we are also releasing a technical report and source code.Improving GANs (code). First, as mentioned above GANs are a very promising family of generative models because, unlike other methods, they produce very clean and sharp images and learn codes that contain valuable information about these textures. However, GANs are formulated as a game between two networks and it is important (and tricky!) to keep them in balance: for example, they can oscillate between solutions, or the generator has a tendency to collapse. In this work, Tim Salimans, Ian Goodfellow, Wojciech Zaremba and colleagues have introduced a few new techniques for making GAN training more stable. These techniques allow us to scale up GANs and obtain nice 128x128 ImageNet samples:Our CIFAR-10 samples also look very sharp - Amazon Mechanical Turk workers can distinguish our samples from real data with an error rate of 21.3% (50% would be random guessing):In addition to generating pretty pictures, we introduce an approach for semi-supervised learning with GANs that involves the discriminator producing an additional output indicating the label of the input. This approach allows us to obtain state of the art results on MNIST, SVHN, and CIFAR-10 in settings with very few labeled examples. On MNIST, for example, we achieve 99.14% accuracy with only 10 labeled examples per class with a fully connected neural network   a result that s very close to the best known results with fully supervised approaches using all 60,000 labeled examples. This is very promising because labeled examples can be quite expensive to obtain in practice.Generative Adversarial Networks are a relatively new model (introduced only two years ago) and we expect to see more rapid progress in further improving the stability of these models during training.Improving VAEs (code). In this work Durk Kingma and Tim Salimans introduce a flexible and computationally scalable method for improving the accuracy of variational inference. In particular, most VAEs have so far been trained using crude approximate posteriors, where every latent variable is independent. Recent extensions have addressed this problem by conditioning each latent variable on the others before it in a chain, but this is computationally inefficient due to the introduced sequential dependencies. The core contribution of this work, termed inverse autoregressive flow (IAF), is a new approach that, unlike previous work, allows us to parallelize the computation of rich approximate posteriors, and make them almost arbitrarily flexible.We show some example 32x32 image samples from the model in the image below, on the right. On the left are earlier samples from the DRAW model for comparison (vanilla VAE samples would look even worse and more blurry). The DRAW model was published only one year ago, highlighting again the rapid progress being made in training generative models.InfoGAN (code). Peter Chen and colleagues introduce InfoGAN   an extension of GAN that learns disentangled and interpretable representations for images. A regular GAN achieves the objective of reproducing the data distribution in the model, but the layout and organization of the code space is underspecified   there are many possible solutions to mapping the unit Gaussian to images and the one we end up with might be intricate and highly entangled. The InfoGAN imposes additional structure on this space by adding new objectives that involve maximizing the mutual information between small subsets of the representation variables and the observation. This approach provides quite remarkable results. For example, in the images of 3D faces below we vary one continuous dimension of the code, keeping all others fixed. It\\'s clear from the five provided examples (along each row) that the resulting dimensions in the code capture interpretable dimensions, and that the model has perhaps understood that there are camera angles, facial variations, etc., without having been told that these features exist and are important:The next two recent projects are in a reinforcement learning (RL) setting (another area of focus at OpenAI), but they both involve a generative model component.Curiosity-driven Exploration in Deep Reinforcement Learning via Bayesian Neural Networks (code). Efficient exploration in high-dimensional and continuous spaces is presently an unsolved challenge in reinforcement learning. Without effective exploration methods our agents thrash around until they randomly stumble into rewarding situations. This is sufficient in many simple toy tasks but inadequate if we wish to apply these algorithms to complex settings with high-dimensional action spaces, as is common in robotics. In this paper, Rein Houthooft and colleagues propose VIME, a practical approach to exploration using uncertainty on generative models. VIME makes the agent self-motivated; it actively seeks out surprising state-actions. We show that VIME can improve a range of policy search methods and makes significant progress on more realistic tasks with sparse rewards (e.g. scenarios in which the agent has to learn locomotion primitives without any guidance).Finally, we would like to include a bonus fifth project: Generative Adversarial Imitation Learning (code), in which Jonathan Ho and colleagues present a new approach for imitation learning. Jonathan Ho is joining us at OpenAI as a summer intern. He did most of this work at Stanford but we include it here as a related and highly creative application of GANs to RL. The standard reinforcement learning setting usually requires one to design a reward function that describes the desired behavior of the agent. However, in practice this can sometimes involve expensive trial-and-error process to get the details right. In contrast, in imitation learning the agent learns from example demonstrations (for example provided by teleoperation in robotics), eliminating the need to design a reward function.Popular imitation approaches involve a two-stage pipeline: first learning a reward function, then running RL on that reward. Such a pipeline can be slow, and because it s indirect, it is hard to guarantee that the resulting policy works well. This work shows how one can directly extract policies from data via a connection to GANs. As a result, this approach can be used to learn policies from expert demonstrations (without rewards) on hard OpenAI Gym environments, such as Ant and Humanoid.Generative models are a rapidly advancing area of research. As we continue to advance these models and scale up the training and the datasets, we can expect to eventually generate samples that depict entirely plausible images or videos. This may by itself find use in multiple applications, such as on-demand generated art, or Photoshop++ commands such as \"make my smile wider\". Additional presently known applications include image denoising, inpainting, super-resolution, structured prediction, exploration in reinforcement learning, and neural network pretraining in cases where labeled data is expensive.However, the deeper promise of this work is that, in the process of training generative models, we will endow the computer with an understanding of the world and what it is made up of.',\n",
       " \"We'd like to welcome the latest set of team members to OpenAI (and we're still hiring!):\",\n",
       " \"We're releasing the public beta of OpenAI Gym, a toolkit for developing and comparing reinforcement learning (RL) algorithms. It consists of a growing suite of environments (from simulated robots to Atari games), and a site for comparing and reproducing results.OpenAI Gym is compatible with algorithms written in any framework, such as Tensorflow and Theano. The environments are written in Python, but we'll soon make them easy to use from any language. We originally built OpenAI Gym as a tool to accelerate our own RL research. We hope it will be just as useful for the broader community.If you'd like to dive in right away, you can work through our tutorial. You can also help out while learning by reproducing a result.Reinforcement learning (RL) is the subfield of machine learning concerned with decision making and motor control. It studies how an agent can learn how to achieve goals in a complex, uncertain environment. It's exciting for two reasons:However, RL research is also slowed down by two factors:OpenAI Gym is an attempt to fix both problems.OpenAI Gym provides a diverse suite of environments that range from easy to difficult and involve many different kinds of data. We're starting out with the following collections:Over time, we plan to greatly expand this collection of environments. Contributions from the community are more than welcome.Each environment has a version number (such as Hopper-v0). If we need to change an environment, we'll bump the version number, defining an entirely new task. This ensures that results on a particular environment are always comparable.We've made it easy to upload results to OpenAI Gym. However, we've opted not to create traditional leaderboards. What matters for research isn't your score (it's possible to overfit or hand-craft solutions to particular tasks), but instead the generality of your technique.We're starting out by maintaining a curated list of contributions that say something interesting about algorithmic capabilities. Long-term, we want this curation to be a community effort rather than something owned by us. We'll necessarily have to figure out the details over time, and we'd would love your help in doing so.We want OpenAI Gym to be a community effort from the beginning. We've starting working with partners to put together resources around OpenAI Gym:During the public beta, we're looking for feedback on how to make this into an even better tool for research. If you'd like to help, you can try your hand at improving the state-of-the-art on each environment, reproducing other people's results, or even implementing your own environments. Also please join us in the community chat!\",\n",
       " 'We have two more team updates.We could not be more excited to work with both. Welcome Pieter and Shivon!',\n",
       " \"We've had some fantastic people join over the past few months (and we're still hiring). Welcome, everyone!Also joining us for the summer (and in some cases, to continue the collaboration once they return to their home institution):As a closing note, we get a lot of questions about what we're working on, how we work, and what we're trying to achieve. We're not being intentionally mysterious; we've just been busy launching the organization (and finding awesome people to help us do so!).We're currently focused on unsupervised learning and reinforcement learning. We should have interesting results to share over the next month or two. A bunch of us will be around ICLR, where we'll likely hold an event of some form. I'll also host a Quora Session in May or June to answer any questions for people we don't meet in Puerto Rico.\",\n",
       " \"OpenAI is a non-profit artificial intelligence research company. Our goal is to advance digital intelligence in the way that is most likely to benefit humanity as a whole, unconstrained by a need to generate financial return. Since our research is free from financial obligations, we can better focus on a positive human impact.We believe AI should be an extension of individual human wills and, in the spirit of liberty, as broadly and evenly distributed as possible. The outcome of this venture is uncertain and the work is difficult, but we believe the goal and the structure are right. We hope this is what matters most to the best in the field.Artificial intelligence has always been a surprising field. In the early days, people thought that solving certain tasks (such as chess) would lead us to discover human-level intelligence algorithms. However, the solution to each task turned out to be much less general than people were hoping (such as doing a search over a huge number of moves).The past few years have held another flavor of surprise. An AI technique explored for decades, deep learning, started achieving state-of-the-art results in a wide variety of problem domains. In deep learning, rather than hand-code a new algorithm for each problem, you design architectures that can twist themselves into a wide range of algorithms based on the data you feed them.This approach has yielded outstanding results on pattern recognition problems, such as recognizing objects in images, machine translation, and speech recognition. But we've also started to see what it might be like for computers to be creative, to dream, and to experience the world.AI systems today have impressive but narrow capabilities. It seems that we'll keep whittling away at their constraints, and in the extreme case they will reach human performance on virtually every intellectual task. It's hard to fathom how much human-level AI could benefit society, and it's equally hard to imagine how much it could damage society if built or used incorrectly.Because of AI's surprising history, it's hard to predict when human-level AI might come within reach. When it does, it'll be important to have a leading research institution which can prioritize a good outcome for all over its own self-interest.We're hoping to grow OpenAI into such an institution. As a non-profit, our aim is to build value for everyone rather than shareholders. Researchers will be strongly encouraged to publish their work, whether as papers, blog posts, or code, and our patents (if any) will be shared with the world. We'll freely collaborate with others across many institutions and expect to work with companies to research and deploy new technologies.OpenAI's research director is Ilya Sutskever, one of the world experts in machine learning. Our CTO is Greg Brockman, formerly the CTO of Stripe. The group's other founding members are world-class research engineers and scientists: Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, John Schulman, Pamela Vagata, and Wojciech Zaremba. Pieter Abbeel, Yoshua Bengio, Alan Kay, Sergey Levine, and Vishal Sikka are advisors to the group. OpenAI's co-chairs are Sam Altman and Elon Musk.Sam, Greg, Elon, Reid Hoffman, Jessica Livingston, Peter Thiel, Amazon Web Services (AWS), Infosys, and YC Research are donating to support OpenAI. In total, these funders have committed $1 billion, although we expect to only spend a tiny fraction of this in the next few years.You can follow us on Twitter at @openai.\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(link)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "n_cluster = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.5,min_df=2,stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters =n_cluster ,init='k-means++', max_iter = 100, n_init = 1, verbose =True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration  0, inertia 72.024\n",
      "Iteration  1, inertia 37.824\n",
      "Converged at iteration 1: center shift 0.000000e+00 within tolerance 5.103012e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "    n_clusters=3, n_init=1, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]), array([10, 19, 16], dtype=int64))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " import numpy as np\n",
    "np.unique(km.labels_,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#aggregate text in each cluster\n",
    "text_d={}\n",
    "for i, cluster in enumerate(km.labels_):\n",
    "    oneDocument = text[i]\n",
    "    if cluster not in text_d.keys():\n",
    "        text_d[cluster] = oneDocument\n",
    "    else:\n",
    "        text_d[cluster] += oneDocument\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 1, 2, 1, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 2, 2, 1, 1, 1, 2, 1, 1,\n",
       "       2, 1, 0, 1, 0, 0, 1, 1, 2, 0, 1, 0, 0, 0, 0, 2, 2, 2, 1, 0, 0, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<45x1776 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 7676 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords #collection of stopwords from different languages\n",
    "from nltk.probability import FreqDist\n",
    "from collections import defaultdict\n",
    "from heapq import nlargest\n",
    "from string import punctuation #punctuation since they are considered as tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_stopwords = set(stopwords.words('english') + list(punctuation)) #get stopwords and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "st = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = {}\n",
    "counts = {}\n",
    "\n",
    "for cluster in range(n_cluster):\n",
    "    word_sent = word_tokenize(text_d[cluster].lower())   \n",
    "    word_sent=[word for word in word_sent if word not in _stopwords]\n",
    "    \n",
    "    %stemmedWords=[st.stem(word) for word in word_sent]\n",
    "    freq = FreqDist(word_sent)\n",
    "    keywords[cluster] = nlargest(100,freq,key=freq.get)\n",
    "    counts[cluster]=freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "unique_keys={}\n",
    "for cluster in range(n_cluster):\n",
    "    print(cluster)\n",
    "    other_clusters=list(set(range(n_cluster))-set([cluster]))\n",
    "    keys_other_clusters=set(keywords[other_clusters[0]]).union(set(keywords[other_clusters[1]]))\n",
    "\n",
    "    unique=set(keywords[cluster])-keys_other_clusters\n",
    "\n",
    "    unique_keys[cluster]= nlargest(10, unique, key=counts[cluster].get)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['machine',\n",
       "  'people',\n",
       "  'problems',\n",
       "  'defense',\n",
       "  'attacker',\n",
       "  'paper',\n",
       "  'output',\n",
       "  'everyone',\n",
       "  'event',\n",
       "  'masking'],\n",
       " 1: ['agents',\n",
       "  'environments',\n",
       "  'environment',\n",
       "  'reward',\n",
       "  'rl',\n",
       "  'games',\n",
       "  'actions',\n",
       "  'performance',\n",
       "  'noise',\n",
       "  'algorithm'],\n",
       " 2: ['images',\n",
       "  'data',\n",
       "  'x',\n",
       "  'generative',\n",
       "  'sparse',\n",
       "  'world',\n",
       "  'networks',\n",
       "  'robot',\n",
       "  'min',\n",
       "  'kernels']}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_keys[cluster]= nlargest(10, unique, key=counts[cluster].get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['machine',\n",
       "  'people',\n",
       "  'problems',\n",
       "  'defense',\n",
       "  'attacker',\n",
       "  'paper',\n",
       "  'output',\n",
       "  'everyone',\n",
       "  'event',\n",
       "  'masking'],\n",
       " 1: ['agents',\n",
       "  'environments',\n",
       "  'environment',\n",
       "  'reward',\n",
       "  'rl',\n",
       "  'games',\n",
       "  'actions',\n",
       "  'performance',\n",
       "  'noise',\n",
       "  'algorithm'],\n",
       " 2: ['images',\n",
       "  'data',\n",
       "  'x',\n",
       "  'generative',\n",
       "  'sparse',\n",
       "  'world',\n",
       "  'networks',\n",
       "  'robot',\n",
       "  'min',\n",
       "  'kernels']}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Regan CollinsBlockedUnblockFollowFollowingCEO at AzuronautNov 27Workplace by Facebook and SharePoint   Better Together.Image credit: Workplace by FacebookTwo of the world s biggest brands in technology and business are proving that working together provides the best solution for their Enterprise clients.The theory writers have not had time to put pen to paper on this unwritten and somewhat undisclosed relationship, but as the CEO of Azuronaut   a Workplace by Facebook and a Microsoft Gold partner   I ve been sitting in the catbird seat.From my vantage point it s clear that both Microsoft and Workplace by Facebook can give big benefit to businesses by allowing organisations to use the best of both platforms.Whole new worldOrganisations previously tried to meet their needs by using  one size fits  all software solutions. Now   with SaaS implementation times and pricing methodologies   businesses can use multiple on-point cloud technologies, integrate them and reap the benefits. So when I meet with large enterprises to advise them on business transformation and better ways of working, the number one question I get asked is  where can I give my employees one place to perform all tasks and one place to access all systems and tools  In the nineties, people sought to answer this question by linking IT systems together using Single Sign On (SSO) and developing purpose-built SharePoint Sites to bring employees together and help them collaborate. Despite heavy investment these solutions never fully worked or felt right because SharePoint wasn t built with engagement as it s core competency!Today, I put forward a very simple but powerful idea: use tools for the purpose they were built for. Office 365 for productivity applications. SharePoint for document storage and management. Workplace by Facebook for engagement.Tools for the purpose they were built forFacebook has entered the B2B market with Workplace and quickly proven that the familiarity of Facebook can link an organisation from the CEO to the shop floor, promote bottom up decision making and create an innovative company culture.For the first time staff have been asked to pick up their phones at work, open a consumer-like application and engage in an open dialogue with their management. The results have been eye watering. Organisations have achieved adoption rates of 80%+ and CEO s at Fortune 500s like Starbucks have shared use cases on how Workplace has transformed work culture and processes over night.At Starbucks a process which would have normally taken weeks, if not months, happened in 24 hours thanks to Workplace. Click here to learn more.Microsoft on the other hand also has a mature suite of products that are familiar and used in most large organisations. Office 365 brings capabilities well beyond Word, Excel and PowerPoint that are hard to replicate and SharePoint is second-to-none for document storage, co-authoring, version control and workflows.As with Workplace its products also meet the most stringent security and governance standards which are required by enterprise corporations.One point of entryAt my company Azuronaut we regularly implement the two platforms through one single connected experience. Workplace becomes the entry point where employees login each morning and engage directly with colleagues. Microsoft s tools and applications then operate behind the scenes allowing teams to collaborate on documents, store files and use purpose-built applications and workflows.So if a user is publishing top-down content like the annual CFO announcement or the arrival of a new CEO they can get the appropriate approvals using built-in workflows and sign-off in SharePoint. They can then publish to the entire organisation using Workplace. It s the same process as a traditional Newspaper sharing its articles on consumer Facebook. Facebook becomes the point of likes, comments and engagement just like Workplace.What s more is that forward thinking companies are now starting to move complicated but business critical systems to the background. These systems then operate as the brains behind a more engaging front end of consumer like applications and artificially intelligent chatbots.Example I   SharePoint BotUsers can search for SharePoint documents from within Workplace using the Azuronaut SharePoint Bot. Clients use this Bot to easily access content stored across multiple SharePoint sites and open and edit documents from within Workplace. An employee can easily search for the document or file they are looking for, see a preview and open the right version to collaborate.Example II   Outlook BotWe have clients who now book all of their internal meetings through an Outlook Bot on Workplace by @ mentioning their colleagues in Groups and Chat. In the background, the intelligent bot finds availability across multiple calendars and geographies, creates the calendar event and automatically sends out invitations. Reminders are then automatically sent in Workplace Chat before the start of an event and busy attendees can notify others of their status   such as  running 5 minutes late  or  I m on my way .Example III   Automated User and Group ManagementWe have clients who automate User and Group Management in Workplace using their organisational structures in Azure AD. This means employees are instantly able to operate from Workplace when they join a company and access the information they need to do their jobs. Administrators can then stay on top of compliance by preventing those who have changed departments or left an organisation from accessing unauthorised information.SummaryIn all of the above cases, Workplace becomes the point of entry for users and Microsoft operates as the brains and workflow behind the interaction.The power comes from a unique partnership where two technologies work at what they do best and deliver flawless results to an organisation and its employees.Simply put, these two platforms are better together.My readers and I would love to hear your thoughts!Please comment on the section below if you d like to contribute to this conversation Want to connect  Email: regan@azuronaut.com LinkedIn: https://www.linkedin.com/in/regancollinsAzuronautOne clap, two clap, three clap, forty By clapping more or less, you can signal to us which stories really stand out.135BlockedUnblockFollowFollowingRegan CollinsCEO at AzuronautFollowAzuronautPioneering the Future of Work  '"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import Request, urlopen\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from bs4 import BeautifulSoup  #parse html page\n",
    "url = \"https://futureofwork.azuronaut.com/workplace-by-facebook-and-sharepoint-better-together-b703203211e7\"\n",
    "#get the webpage\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})  #tell that the scraper is a web browser\n",
    "webpage = urlopen(req).read().decode('utf8', 'ignore')\n",
    "soup = BeautifulSoup(webpage, \"lxml\")\n",
    "\n",
    "#special characters are due to encoding\n",
    "text = text.encode('ascii', errors='replace').decode('ascii')\n",
    "text.replace(\"?\", \" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=10, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## k-neighbours classifier\n",
    "classifier = KNeighborsClassifier(n_neighbors=10)\n",
    "classifier.fit(X,km.labels_) #training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#represent article with tf_IDF form\n",
    "test = vectorizer.transform([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x1776 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 156 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
